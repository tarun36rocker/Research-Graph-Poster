{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b972e1d",
   "metadata": {},
   "source": [
    "# Extract pagewise research paper contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b84024fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "752da65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision diffusers transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4298ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path='8.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0021b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Text:\n",
      "Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding Xiner Li 1 Yulai Zhao 2 Chenyu Wang 3 Gabriele Scalia 4 Gokcen Eraslan4 Surag Nair4 Tommaso Biancalani 4 Aviv Regev 4∗ Sergey Levine5∗ Masatoshi Uehara 4∗ 1Texas A&M University 2 Princeton University 3 MIT 4 Genentech 5 UC Berkeley Abstract Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely -generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require “differentiable” proxy models (e.g., classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (e.g., classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation. The code is available at https://github.com/masa-ue/SVDD. 1 Introduction Diffusion models have gained popularity as powerful generative models. Their applications extend beyond image generation to include natural language generation (Sahoo et al., 2024; Shi et al., 2024; Lou et al., 2023), molecule generation (Jo et al., 2022; Vignac et al., 2022), and biological (DNA, RNA, protein) sequence generation (Avdeyev et al., 2023; Stark et al., 2024), and in each of these domains diffusion models have been shown to be very effective at capturing complex natural distributions. However, in practice, we might not only want to generate realistic samples, but to produce samples that optimize specific downstream reward functions while preserving naturalness by leveraging pre-trained models. For example, in computer vision, we might aim to generate natural images with high aesthetic and alignment scores. In drug discovery, we may seek to generate valid molecules with high QED/SA/docking scores (Lee et al., 2023; Jin et al., 2018) or natural RNAs (such as mRNA vaccines (Cheng et al., 2023)) with high translational efficiency and stability (Castillo-Hair and Seelig, 2021; Asrani et al., 2018), and natural DNAs with high cell-specificity (Gosai et al., 2023; Taskiran et al., 2024; Lal et al., 2024). The optimization of downstream reward functions using pre-trained diffusion models has been approached in various ways. In our work, we focus on non-fine-tuning-based methods because fine- tuning generative models (e.g., when using classifier-free guidance (Ho et al., 2020) or RL-based fine- ∗regev.aviv@gene.com, svlevine@eecs.berkeley.edu , uehara.masatoshi@gene.com Preprint. Under review. arXiv:2408.08252v1  [cs.LG]  15 Aug 2024 \n",
      "\n",
      "Page 2 Text:\n",
      "Table 1: A comparison of our method with a number of prior approaches. “Non-differentiable” refers to the method’s capability to operate without requiring differentiable proxy models. “No Training” means that no additional training of the diffusion model is required as long as we have access to the reward feedback. No fine-tuning Non-differentiable No Training Classifier guidance (Dhariwal and Nichol, 2021) ✓ DPS (Chung et al., 2022) ✓ ✓ Classifier-free (Ho and Salimans, 2022) ✓ RL fine-tuning ✓ SVDD-MC (Ours) ✓ ✓ SVDD-PM (Ours) ✓ ✓ ✓ tuning (Black et al., 2023; Fan et al., 2023; Uehara et al., 2024; Clark et al., 2023; Prabhudesai et al., 2023)) often becomes computationally intensive, especially as pre-trained generative models grow larger in the era of “foundation models”. Although classifier guidance and its variants (e.g., Dhariwal and Nichol (2021); Song et al. (2020); Chung et al. (2022); Bansal et al. (2023); Ho et al. (2022)) have shown success as non-fine-tuning methods in these settings, they face significant challenges. Firstly, as they would require constructing differentiable proxy models, they cannot directly incorporate useful domain-specific non-differentiable features (e.g., molecular/protein descriptors (van Westen et al., 2013; Ghiringhelli et al., 2015; Emonts and Buyel, 2023)) or non-differentiable reward feedback (e.g., physics-based simulations such as Vina and Rosetta (Trott and Olson, 2010; Alhossary et al., 2015; Alford et al., 2017)). This limitation also hinders the application of current classifier guidance methods to recently developed discrete diffusion models (Austin et al., 2021; Campbell et al., 2022; Sahoo et al., 2024; Shi et al., 2024; Lou et al., 2023) in a principled manner (i.e., without transforming the discrete space into the Euclidean space). Additionally, gradient computation can be memory-intensive with large proxy models. A C C G U A M C G M A M M G M xt xt−1 xT x0 v(xt−1) := E[r(x0)|xt−1] M M M M M A M M G M A M G M A M M G M C A M G M U 2.4 4.2 3.5 RNA Translational efficiency r(x0) Figure 1: Summary of SVDD. The notation v denotes value functions that predict reward r(x0) (at time 0) from states at time t −1. SVDD involves two steps: (1) generating multiple noisy states from pre-trained models and (2) selecting the state with the highest value according to the value function. We propose a novel method, SVDD (Soft Value- based Decoding in Diffusion models), for op- timizing downstream reward functions in diffu- sion models (summarized in Figure 1) to address the aforementioned challenges. Inspired by re- cent literature on RL-based fine-tuning (Uehara et al., 2024), we first introduce soft value func- tions that serve as look-ahead functions, indicat- ing how intermediate noisy samples lead to high rewards in the future of the diffusion denoising process. After learning (or approximating) these value functions, we present a new inference-time technique, SVDD, which obtains multiple noisy states from the policy (i.e., denoising map) of pre- trained diffusion models and selects the sample with the highest value function at each time step. Specifically, we introduce two algorithms (SVDD- MC and SVDD-PM) depending on how we es- timate value functions. Notably, the SVDD-PM approach, leveraging the characteristics of diffu- sion models (e.g., utilizing the forward process in diffusion models to directly map t to 0 in terms of expectation in Figure 1), requires no additional training as long as we have access to the reward feedback. Our contributions are summarized as follows (also see Table 1). We propose a novel technique for optimizing downstream reward functions in pre-trained diffusion models that eliminates the need to construct differentiable proxy models and avoids the need to fine-tune the generative model itself. The first property allows for the use of non-differentiable reward feedback, which is common in many scientific fields, and makes our method applicable to recent discrete diffusion models without any modification. The second property addresses the high computational cost associated with fine-tuning diffusion models. We demonstrate the effectiveness of our methods across various domains, including image generation, molecule generation, and DNA/RNA generation. 2 \n",
      "\n",
      "Page 3 Text:\n",
      "2 Related Works Here, we summarize related work. We first outline methods relevant to our goal, categorizing them based on whether they involve fine-tuning. We then discuss related directions, such as discrete diffusion models, where our method excels, and decoding in autoregressive models. Non-fine-tuning methods. There are primarily two methods for optimizing downstream functions in diffusion models without fine-tuning. • Classifier guidance (Dhariwal and Nichol, 2021; Song et al., 2020): It has been widely used to condition pre-trained diffusion models without fine-tuning. Although these methods do not originally focus on optimizing reward functions, they can be applied for this purpose (Uehara et al., 2024, Section 6.2). In this approach, an additional derivative of a certain value function is incorporated into the drift term (mean) of pre-trained diffusion models during inference. Subsequent variants (e.g., Chung et al. (2022); Ho et al. (2022); Bansal et al. (2023); Guo et al. (2024); Wang et al. (2022); Yu et al. (2023)) have been proposed to simplify the learning of value functions. However, these methods require constructing differentiable models, which limits their applicability to non-differentiable features/reward feedbacks commonly encountered in scientific domains as mentioned in Section 1. Additionally, this approach cannot be directly extended to discrete diffusion models in a principle way 2. Our approach aims to address these challenges. • Best-of-N: The naive way is to generate multiple samples and select the top samples based on the reward functions, known as best-of-K in the literature on (autoregressive) LLMs (Stiennon et al., 2020; Nakano et al., 2021; Touvron et al., 2023; Beirami et al., 2024; Gao et al., 2023). This approach is significantly less efficient than ours, as our method uses soft-value functions that predict how intermediate noisy samples lead to high rewards in the future. We validate this experimentally in Section 6. Fine-tuning of diffusion models. Several methods exist for fine-tuning generative models to optimize downstream reward functions, such as classifier-free guidance (Ho and Salimans, 2022) and RL-based fine-tuning (Fan et al., 2023; Black et al., 2023)/its variants (Dong et al., 2023). However, these approaches often come with caveats, including high computational costs and the risk of easily forgetting pre-trained models. In our work, we propose an inference-time technique that eliminates the need for fine-tuning generative models, similar to classifier guidance. Discrete diffusion models. Based on seminal works Austin et al. (2021); Campbell et al. (2022), recent work on masked diffusion models (Lou et al., 2023; Shi et al., 2024; Sahoo et al., 2024) has demonstrated their strong performance in natural language generation. Additionally, they have been applied to biological sequence generation (e.g., DNA, protein sequences in Campbell et al. (2024); Sarkar et al. (2024)). In these cases, the use of diffusion models over autoregressive models is particularly apt, given that many biological sequences ultimately adopt complex three-dimensional structures. We also note that ESM3 (Hayes et al., 2024), a widely recognized foundational model in protein sequence generation, bears similarities to masked diffusion models. Despite its significance, it cannot be integrated with standard classifier guidance because adding a continuous gradient to a discrete objective is not inherently valid. Unlike standard classifier guidance, our algorithm can be seamlessly applied to discrete diffusion models. Decoding in autoregressive models with rewards. The decoding strategy, which dictates how sentences are generated from the model, is a critical component of text generation in autoregressive language models (Wu et al., 2016; Chorowski and Jaitly, 2016; Leblond et al., 2021). Recent studies have explored inference-time techniques for optimizing downstream reward functions Dathathri et al. (2019); Yang and Klein (2021); Qin et al. (2022); Mudgal et al. (2023); Zhao et al. (2024); Han et al. (2024). While there are similarities between these works and ours, to the best of our knowledge, no prior work has extended such methodologies to diffusion models. Furthermore, our approach leverages characteristics unique to diffusion models that are not present in autoregressive models such as SVDD-PM. 2We note that a notable exception has been recently proposed (Nisonoff et al., 2024). However, our method can be applied to both continuous and discrete diffusion models in a unified manner. 3 \n",
      "\n",
      "Page 4 Text:\n",
      "3 Preliminaries and Goal In this section, we describe the standard method for training diffusion models and outline the objective of our work: optimizing downstream reward functions given pre-trained diffusion models. 3.1 Diffusion Models In diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020), our goal is to learn a sampler p(x|c) ∈[C →∆(X)] given data consisting of (x, c) ∈X × C. For instance, in text-to-image diffusion models, c is a text, and x is an image. In protein inverse folding, c is a backbone protein structure, and x is a protein sequence. The training process for a standard diffusion model is summarized as follows. First, we introduce a (fixed) forward process qt : X →∆(X). Now, after introducing the forward process, we aim to learn a backward process: {pt} where each pt is X × C →∆(X) so that the induced distributions induced by the forward process and backward process match marginally. For this purpose, by parametrizing the backward processes with θ ∈Rd, we typically use the following loss function: Ex1,··· ,xT ∼q(·|x0) \" −log p0(x0|x1) + T X t=1 KL(qt(· | xt−1)∥pt(· | xt+1, c; θ)) + KL(qT (·)∥pT (·)) # , which is derived from the variational lower bound of the negative likelihood (i.e., ELBO). Here are two examples of concrete parameterizations. Let αt ∈R be a noise schedule. Example 1 (Continuous space). When X is Euclidean space, we typically use the Gaussian distribu- tion qt(· | x) = N(√αtx, (1 −αt)). Then, the backward process is parameterized as N \u0012√αt(1 −¯ αt+1)xt + √αt−1(1 −αt)ˆ x0(xt, c; θ) 1 −¯ αt , (1 −αt)(1 −¯ αt−1) 1 −¯ αt \u0013 , where ¯ αt = Qt i=1 αi. Here, ˆ x0(xt, c; θ) is a neural network that predicts x0 from xt (i.e., Eq[x0|xt, c]). Example 2 (Discrete space in Sahoo et al. (2024); Shi et al. (2024)). Let X be a space of one-hot column vectors {x ∈{0, 1}K : PK i=1 xi = 1}, and Cat(π) be the categorical distribution over K classes with probabilities given by π ∈∆K where ∆K denotes the K-simplex. A typical choice is qt(· | x) = Cat(αtx + (1 −αt)m) where m = [0, · · · , 0, Mask]. Then, the backward process is parameterized as ( Cat(xt), if xt ̸= m Cat \u0010 (1−αt−1)m+(αt−1−αt)ˆ x0(xt,c;θ) 1−αt \u0011 , if xt = m. Here, ˆ x0(xt, c; θ) is a neural network that predicts x0 from xt. Note that when considering a sequence of L tokens (x1:L), we use the direct product: pt(x1:L t |x1:L t+1, c) = QL l=1 pt(xl t|x1:L t+1, c). After learning the backward process, we can sample from a distribution that emulates training data distribution (i.e., p(x|c)) by sequentially sampling {pt}0 t=T from t = T to t = 0. Notation. The notation δa denotes a Dirac delta distribution centered at a. The notation ∝indicates that the distribution is equal up to a normalizing constant. With slight abuse of notation, we often denote pT (·|·, ·) by pT (·). 3.2 Objective: Generating Samples with High Rewards While Preserving Naturalness We consider a scenario where we have a pre-trained diffusion model, which is trained using the loss function explained in Section 3.1. These pre-trained models are typically designed to excel at characterizing the natural design space (e.g., image space, biological space, or chemical space) by emulating the extensive training dataset. Our work focuses on obtaining samples that also optimize downstream reward functions r : X →R (e.g., QED and SA in molecule generation) while maintaining the naturalness by leveraging pre-trained diffusion models. We formalize this goal as follows. 4 \n",
      "\n",
      "Page 5 Text:\n",
      "Given a pre-trained model {ppre t }0 t=T , we denote the induced distribution by ppre(x|c) (i.e., R {Q1 t=T +1 ppre t−1(xt−1|xt, c)}dx1:T ). We aim to sample from the following distribution: p(α)(x|c) := argmax p∈[C→∆(X)] Ex∼p(·|c)[r(x)] | {z } term (a) −α KL(p(· | c)∥ppre(·|c)) | {z } term(b) ∝exp(r(x)/α)ppre(x|c). Here, the term (a) is introduced to optimize reward function, while the term (b) is used to maintain the naturalness of the generated samples. Existing methods. Several existing approaches target this goal (or its variant), as discussed in Section 2, including classifier guidance, fine-tuning (RL-based or classifier-free), and Best-of-N. In our work, we focus on non-fine-tuning-based methods, such as classifier guidance and Best-of-N. Specifically, we aim to address the limitations of these methods: the requirement for differentiable proxy models in classifier guidance and the inefficiency of Best-of-N. 4 Soft Value-Based Decoding in Diffusion Models In this section, we first present the motivation behind developing our new algorithm. We then introduce our algorithm, which satisfies desired properties, i.e., the lack of need for fine-tuning or constructing differentiable models. 4.1 Key Observation We introduce several key concepts. First, for t ∈[T + 1, · · · , 1], we define the soft value function: vt−1(·) := α log Ex0∼ppre(x0|xt−1) \u0014 exp \u0012r(x0) α \u0013 |xt−1 = · \u0015 , where E{ppre}[·] is induced by {ppre t (·|xt+1, c)}0 t=T . This value function represents the expected future reward at t = 0 from the intermediate noisy state at t −1. Next, we define the following soft optimal policy (denoising process) p⋆,α t−1 : X × C →∆(X) weighted by value functions vt−1 : X →R: p⋆,α t−1(·|xt, c) = ppre t−1(·|xt, c) exp(vt−1(·)/α) R ppre t−1(x|xt, c) exp(vt−1(x)/α)dx. Here, we refer to vt as soft value functions, p⋆,α t as soft optimal policies, respectively, because they literally correspond to soft value functions and soft optimal policies where we embed diffusion models into entropy-regularized MDPs, as demonstrated in Uehara et al. (2024). With this preparation in mind, we utilize the following key observation: Theorem 1 (From Theorem 1 in Uehara et al. (2024)). The distribution induced by {p⋆,α t (·|xt+1, c)}0 t=T is the target distribution p(α)(x|c), i.e., p(α)(x0|c) = R nQ1 t=T +1 p⋆,α t−1(xt−1|xt, c) o dx1:T . While Uehara et al. (2024) presents this theorem, they use it primarily to interpret their RL-based fine-tuning introduced in Fan et al. (2023); Black et al. (2023). In contrast, our work explores how to convert this into a new fine-tuning-free optimization algorithm. Our motivation for a new algorithm. Theorem 1 states that if we can hypothetically sample from {p⋆,α t }0 t=T , we can sample from the target distribution p(α). However, there are two challenges in sampling from each p⋆,α t−1: 1. The soft-value function vt−1 in p⋆,α t−1 is unknown. 2. It is unnormalized (i.e., calculating the normalizing constant is often hard). 5 \n",
      "\n",
      "Page 6 Text:\n",
      "We address the first challenge later in Section 4.3. Assuming the first challenge is resolved, we consider how to tackle the second challenge. A natural approach is to use importance sampling (IS): p⋆,α t−1(·|xt, c) ≈ M X m=1 w⟨m⟩ t−1 P j w⟨j⟩ t−1 δx⟨m⟩ t−1, {x⟨m⟩ t−1}M m=1 ∼ppre t−1(· | xt, c), where w⟨m⟩ t−1 := exp(vt(x⟨m⟩ t−1)/α). Thus, we can approximately sample from p⋆,α t−1(·|xt, c) by obtaining multiple (M) samples from pre-trained diffusion models and selecting the sample based on an index, which is determined by sampling from the categorical distribution with mean {w⟨m⟩ t−1/ P j w⟨j⟩ t−1}⟨M⟩ m=1. Difference over Best-of-N. Best-of-N, which generates multiple samples and selects the highest reward sample, is technically considered IS where the proposal distribution is the entire ppre(x0|c) = R Q t{ppre t (xt−1 | xt, c)}dx1:T . However, the use of importance sampling in our algorithm differs significantly, as we apply it at each time step to approximate each soft-optimal policy. 4.2 Inference-Time Algorithm Algorithm 1 SVDD (Soft Value-Based Decoding in Diffusion Models) 1: Require: Estimated soft value function {ˆ vt}0 t=T (refer to Algorithm 2 or Algorithm 3), pre- trained diffusion models {ppre t }0 t=T , hyperparameter α ∈R 2: for t ∈[T + 1, · · · , 1] do 3: Get M samples from pre-trained polices {x⟨m⟩ t−1}M m=1 ∼ppre t−1(·|xt), and for each m, calculate w⟨m⟩ t−1 := exp(ˆ vt−1(x⟨m⟩ t−1)/α) 4: xt−1 ←x⟨ζt−1⟩ t−1 after selecting an index: ζt−1 ∼Cat \u001a w⟨m⟩ t−1 PM j=1 w⟨j⟩ t−1 \u001bM m=1 ! , 5: end for 6: Output: x0 Now, we introduce our algorithm, which leverages the observation, in Algorithm 1. Our algorithm is an iterative sampling method that integrates soft value functions into the standard inference procedure of pre-trained diffusion models. Each step is designed to approximately sample from a value-weighted policy {p⋆,α t }0 t=T . We have several important things to note. • When α = 0, Line 4 corresponds to ζt−1 = argmaxm∈[1,··· ,M] ˆ vt−1(x⟨m⟩ t−1). In practice, we often recommend this choice. • A typical choice we recommend is M = [5, · · · , 20]. The performance with varying M values will be discussed in Section 6. • Line 3 can be computed in parallel at the expense of additional memory. • When the normalizing constant can be calculated relatively easily (e.g., in discrete diffusion with small K, L), we can directly sample from {p⋆,α t }0 t=T . The remaining question is how to obtain the soft value function, which we address in the next section. 4.3 Learning Soft Value Functions Next, we describe how to learn soft value functions vt(x) in practice. We propose two main approaches: the Monte Carlo regression approach and the posterior mean approximation approach. Monte Carlo regression. Here, we use the following approximation v′ t as vt where v′ t(·) := Ex0∼ppre(x0|xt)[r(x0)|xt = ·]. 6 \n",
      "\n",
      "Page 7 Text:\n",
      "This is based on vt(xt) := α log Ex0∼ppre(x0|xt)[exp(r(x0)/α)|xt] ≈log(exp(Ex0∼ppre(x0|xt)[r(x0)|xt])) = v′ t(xt). (1) By regressing r(x0) onto xt, we can learn v′ t as in Algorithm 2. Combining this with Algorithm 1, we refer to the entire optimization approach as SVDD-MC. Algorithm 2 Value Function Estimation Using Monte Carlo Regression 1: Require: Pre-trained diffusion models, reward r : X →R, function class Φ : X × [0, T] →R. 2: Collect datasets {x(s) T , · · · , x(s) 0 }S s=1 by rolling-out {ppre t }0 t=T from t = T to t = 0. 3: ˆ v′ = argminf∈Φ PT t=0 PS s=1{r(x(s) 0 ) −f(x(s) t , t)}2. 4: Output: ˆ v′ Note that technically, without approximation introduced in (1), we can estimate vt by regressing exp(r(x0)/α) onto xt based on the original definition. This approach may work in many cases. However, when α is very small, the scaling of exp(r(·)/α) tends to be excessively large. Due to this concern, we generally recommend using using Algorithm 2. Remark 1 (Another way of constructing datasets). We have used the backward process in pre-trained diffusion models to collect training datasets in Algorithm 2. Given that the backward and forward processes are designed to match marginally during the training of diffusion models as discussed in Section 3.1, we can also use the forward process in pre-trained diffusion models to construct datasets. Remark 2 (Another way of learning value functions). Technically, another method for learning value functions is available such as soft-Q-learning (Section B), by leveraging soft-Bellman equations in diffusion models Uehara et al. (2024, Section 3) (as known in standard RL contexts (Levine, 2018; Haarnoja et al., 2017; Geist et al., 2019)). However, since we find Monte Carlo approaches to be more stable, we recommend them over soft-Q-learning. Posterior mean approximation. Here, recalling we use ˆ x0(xt) (approximation of Ex0∼ppre(xt)[x0|xt]) when training pre-trained diffusion models in Section 3.1, we perform the following approximation: vt(x) := α log Ex0∼ppre(x0|xt)[exp(r(x0)/α)|xt] ≈α log(exp(r(ˆ x0(xt))/α) = r(ˆ x0(xt)). Then, we can use r(ˆ x0(xt)) as the estimated value function. The advantage of this approach is that no additional training is required as long as we have r. When combined with Algorithm 1, we refer to the entire approach as SVDD-PM. Algorithm 3 Value Function Estimation using Posterior Mean Approximation 1: Require: Pre-trained diffusion models, reward r : X →R 2: Set ˆ v⋄(·, t) := r(ˆ x0(xt = ·), t) 3: Output: ˆ v⋄ Remark 3 (Relation with DPS). In the context of classifier guidance, similar approximations have been employed (e.g., DPS in Chung et al. (2022)). However, the final inference-time algorithms differ significantly, as these methods compute gradients at the end. 5 Advantages, Limitations, and Extensions of SVDD So far, we have detailed our algorithm, SVDD. In this section, we discuss its advantages, limitations, and extensions. 5.1 Advantages No fine-tuning (or no training in SVDD-PM). Unlike classifier-free guidance or RL-based fine- tuning, SVDD does not require any fine-tuning of the generative models. In particular, when using SVDD-PM, no additional training is needed as long as we have r. 7 \n",
      "\n",
      "Page 8 Text:\n",
      "No need for constructing differentiable models. Unlike classifier guidance, SVDD does not require differentiable proxy models, as there is no need for derivative computations. For example, if r is non-differentiable feedback (e.g., physically-based simulations, QED, SA in molecule generation), our method SVDD-PM can directly utilize such feedback without constructing differentiable proxy models. In cases where non-differentiable feedback is costly to obtain, proxy reward models may still be required, but they do not need to be differentiable; thus, non-differentiable features or non- differentiable models based on scientific knowledge (e.g., molecule fingerprints, GNNs) can be leveraged. Similarly, when using SVDD-MC, while a value function model is required, it does not need to be differentiable, unlike classifier guidance. Additionally, compared to approaches that involve derivatives (like classifier guidance or DPS), our algorithm may be more memory efficient at inference time, particularly when M is moderate, and can be directly applied to discrete diffusion models mentioned in Example 2. Proximity to pre-trained models (robust to reward over-optimization). Since samples are consistently generated from pre-trained diffusion policies at each step, they are ensured to remain within the natural space defined by pre-trained diffusion models. This is especially advantageous when rewards to be optimized by SVDD are learned from offline data. In such cases, learned reward functions may be inaccurate for out-of-distribution samples (Uehara et al., 2024). Consequently, conventional fine-tuning methods often suffer from over-optimization (reward hacking) by exploiting these out-of-distribution regions (Fan et al., 2023; Clark et al., 2023). Given that non-natural design spaces typically encompass a significant portion of out-of-distribution regions of the offline data, maintaining proximity to pre-trained models acts as a regularization mechanism against these regions. 5.2 Potential Limitations Memory and computational complexity in inference time. Our approach requires more compu- tational resources (if not parallelized) or memory (if parallelized), specifically M times more than standard inference methods. Proximity to pre-trained models. The proximity to pre-trained models might be a disadvantage if significant changes to the pre-trained models are desired. We acknowledge that RL-based fine-tuning could be more effective for this purpose than our algorithm. 5.3 Extensions Using a likelihood/classifier as a reward. While we primarily consider scenarios where reward models are regression models, by adopting a similar strategy in (Zhao et al., 2024), they can be readily replaced with classifiers or likelihood functions in the context of solving inverse problems (Chung et al., 2022; Bansal et al., 2023). Combination with sequential Monte Carlo. SVDD iterates IS and resampling at each team step locally, but our algorithm can be combined with the more sophisticated global resampling method known as particle filter method (i.e., sequential Monte Carlo) (Del Moral and Doucet, 2014; Kitagawa, 1993; Doucet et al., 2009). Although we do not recommend this approach in practice due to its difficulty in parallelization, we discuss its extension in Section A. Application to fine-tuning. Our SVDD, can also be naturally extended to fine-tuning by generating samples with SVDD, and then using these samples for supervised fine-tuning. 6 Experiments We conduct experiments to assess the performance of our algorithm relative to baselines and its sensitivity to various hyperparameters. We start by outlining the experimental setup, including baselines and models, and then present the results. 8 \n",
      "\n",
      "Page 9 Text:\n",
      "6.1 Settings Methods to compare. We compare the following methods. • Pre-trained models: We generate samples using pre-trained models. • Best-of-N (Nakano et al., 2021): We generate samples from pre-trained models and select the top 1/N samples. This selection is made to ensure that the computational time during inference is approximately equivalent to that of our proposal. • DPS (Chung et al., 2022): DPS is a widely used enhancement of classifier guidance. Although the original work was not designed for discrete diffusion, we employ specific heuristics to adapt it for this purpose (Section C). • SVDD (Ours): We implement SVDD-MC and SVDD-PM. We generally set M = 20 for images and M = 10 for other domains, additionally we set α = 0. Datasets and reward models. We provide details on the pre-trained diffusion models and downstream reward functions used. For further information, refer to Section C. • Images: We use Stable Diffusion v1.5 as the pre-trained diffusion model (T = 50). For downstream reward functions, we use compressibility and aesthetic scores (LAION Aesthetic Predictor V2 in Schuhmann (2022)), as employed in Black et al. (2023); Fan et al. (2023). Note that compressibility is non-differentiable reward feedback. • Molecules: We use GDSS (Jo et al., 2022), trained on ZINC-250k (Irwin and Shoichet, 2005), as the pre-trained diffusion model (T = 1000). For downstream reward functions, we use QED and SA calculated by RDKit, which are non-differentiable feedback. Here, we renormalize SA to (10 −SA)/9 so that a higher value indicates better performance. • DNAs (Enhancers): We use the discrete diffusion model (Sahoo et al., 2024), trained on datasets from Gosai et al. (2023), as our pre-trained diffusion model (T = 128). For the downstream reward function, we use an Enformer model (Avsec et al., 2021) to predict activity in the HepG2 cell line. • RNAs (5’UTRs): We use the discrete diffusion model (Sahoo et al., 2024) as the pre-trained diffusion model (T = 128). For downstream reward functions, we employ a reward model that predicts the mean ribosomal load (MRL) measured by polysome profiling (trained on datasets from Sample et al. (2019)) and stability (trained on datasets from (Agarwal and Kelley, 2022)). 6.2 Results (a) Images: compress- ibility (b) Images: aesthetic score (c) Molecules: QED (d) Molecules: SA (e) Enhancers (f) 5’UTRs: MRL (g) 5’UTRs: stability Figure 2: We show the histogram of generated samples in terms of reward functions. We compare the baselines with our two proposals. The performance is shown in Figure 2, and the generated samples are presented in Figure 3. 9 \n",
      "\n",
      "Page 10 Text:\n",
      "(a) Images: compressibility (b) Images: aesthetic scores (c) Molecules: QED scores (d) Molecules: SA scores (Normalized as (10 − SA)/9 ) Figure 3: We show generated samples from our proposal. For more samples, refer to Section C.2. Overall, our proposal outperforms the baseline methods (Best-of-N and DPS), as evidenced by higher rewards for samples generated in large quantities. More formally, in Section C, we compare the top 10 and 20 quantiles from each algorithm and confirm that SVDD always outperforms the baselines. This indicates that our algorithm can generate high-reward samples that Best-of-N and DPS cannot. • Compared to Best-of-N, although the rewards for generating samples in smaller quantities could be lower with our algorithm, this is expected because our algorithm generates samples with high likelihood ppre(x|c) in pre-trained diffusion models, but with possibly lower rewards. • Compared to DPS, our algorithm consistently outperforms. Notably, in molecular generation, DPS is highly ineffective due to the non-differentiable nature of the original feedback. The superiority of our two proposals (SVDD-MC or SVDD-PM) appears to be domain-dependent. Generally, SVDD-PM may be more robust since it does not require additional learning (i.e., it directly utilizes reward feedback). The performance of SVDD-MC depends on the success of value function learning, which is discussed in Section C. Figure 4: Performance of our algorithm SVDD as M varies, for image generation while op- timizing the aesthetic score. Ablation studies in terms of M. We plot the performance as M varies. The performance gradually reaches a plateau as M increases. This tendency is seen in all other domains. 7 Conclusion We propose a novel inference-time algorithm, SVDD, for optimizing downstream reward functions in pre-trained diffusion models that eliminate the need to construct differentiable proxy models. In future work, we plan to conduct experiments in other domains, such as protein sequence optimization (Gruver et al., 2023; Alamdari et al., 2023; Watson et al., 2023) or controllable 3D molecule generation (Xu et al., 2023). 10 \n",
      "\n",
      "Page 11 Text:\n",
      "References Agarwal, V. and D. R. Kelley (2022). The genetic and biochemical determinants of mrna degradation rates in mammals. Genome biology 23(1), 245. Alamdari, S., N. Thakkar, R. van den Berg, A. X. Lu, N. Fusi, A. P. Amini, and K. K. Yang (2023). Protein generation with evolutionary diffusion: sequence is all you need. bioRxiv, 2023–09. Alford, R. F., A. Leaver-Fay, J. R. Jeliazkov, M. J. O’Meara, F. P. DiMaio, H. Park, M. V. Shapovalov, P. D. Renfrew, V. K. Mulligan, K. Kappel, et al. (2017). The rosetta all-atom energy function for macromolecular modeling and design. Journal of chemical theory and computation 13(6), 3031–3048. Alhossary, A., S. D. Handoko, Y. Mu, and C.-K. Kwoh (2015). Fast, accurate, and reliable molecular docking with quickvina 2. Bioinformatics 31(13), 2214–2216. Asrani, K. H., J. D. Farelli, M. R. Stahley, R. L. Miller, C. J. Cheng, R. R. Subramanian, and J. M. Brown (2018). Optimization of mrna untranslated regions for improved expression of therapeutic mrna. RNA biology 15(6), 756–762. Austin, J., D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg (2021). Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems 34, 17981–17993. Avdeyev, P., C. Shi, Y. Tan, K. Dudnyk, and J. Zhou (2023). Dirichlet diffusion score model for biological sequence generation. arXiv preprint arXiv:2305.10699. Avsec, ˇ Z., V. Agarwal, D. Visentin, J. R. Ledsam, A. Grabska-Barwinska, K. R. Taylor, Y. Assael, J. Jumper, P. Kohli, and D. R. Kelley (2021). Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods 18(10), 1196–1203. Bansal, A., H.-M. Chu, A. Schwarzschild, S. Sengupta, M. Goldblum, J. Geiping, and T. Goldstein (2023). Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 843–852. Beirami, A., A. Agarwal, J. Berant, A. D’Amour, J. Eisenstein, C. Nagpal, and A. T. Suresh (2024). Theoretical guarantees on the best-of-n alignment policy. arXiv preprint arXiv:2401.01879. Black, K., M. Janner, Y. Du, I. Kostrikov, and S. Levine (2023). Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301. Campbell, A., J. Benton, V. De Bortoli, T. Rainforth, G. Deligiannidis, and A. Doucet (2022). A continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems 35, 28266–28279. Campbell, A., J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola (2024). Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997. Castillo-Hair, S. M. and G. Seelig (2021). Machine learning for designing next-generation mrna therapeutics. Accounts of Chemical Research 55(1), 24–34. Cheng, F., Y. Wang, Y. Bai, Z. Liang, Q. Mao, D. Liu, X. Wu, and M. Xu (2023). Research advances on the stability of mrna vaccines. Viruses 15(3), 668. Chorowski, J. and N. Jaitly (2016). Towards better decoding and language model integration in sequence to sequence models. arXiv preprint arXiv:1612.02695. Chung, H., J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye (2022). Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687. Clark, K., P. Vicol, K. Swersky, and D. J. Fleet (2023). Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400. 11 \n",
      "\n",
      "Page 12 Text:\n",
      "Dathathri, S., A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu (2019). Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv:1912.02164. Del Moral, P. and A. Doucet (2014). Particle methods: An introduction with applications. In ESAIM: proceedings, Volume 44, pp. 1–46. EDP Sciences. Dey, R. and F. M. Salem (2017). Gate-variants of gated recurrent unit (gru) neural networks. In 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS), pp. 1597–1600. IEEE. Dhariwal, P. and A. Nichol (2021). Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34, 8780–8794. Dong, H., W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang (2023). Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767. Doucet, A., A. M. Johansen, et al. (2009). A tutorial on particle filtering and smoothing: Fifteen years later. Handbook of nonlinear filtering 12(656-704), 3. Emonts, J. and J. F. Buyel (2023). An overview of descriptors to capture protein properties–tools and perspectives in the context of qsar modeling. Computational and Structural Biotechnology Journal 21, 3234–3247. Fan, Y., O. Watkins, Y. Du, H. Liu, M. Ryu, C. Boutilier, P. Abbeel, M. Ghavamzadeh, K. Lee, and K. Lee (2023). DPOK: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381. Gao, L., J. Schulman, and J. Hilton (2023). Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 10835–10866. PMLR. Geist, M., B. Scherrer, and O. Pietquin (2019). A theory of regularized markov decision processes. In International Conference on Machine Learning, pp. 2160–2169. PMLR. Ghiringhelli, L. M., J. Vybiral, S. V. Levchenko, C. Draxl, and M. Scheffler (2015). Big data of materials science: critical role of the descriptor. Physical review letters 114(10), 105503. Gosai, S. J., R. I. Castro, N. Fuentes, J. C. Butts, S. Kales, R. R. Noche, K. Mouri, P. C. Sabeti, S. K. Reilly, and R. Tewhey (2023). Machine-guided design of synthetic cell type-specific cis-regulatory elements. bioRxiv. Gruver, N., S. Stanton, N. C. Frey, T. G. Rudner, I. Hotzel, J. Lafrance-Vanasse, A. Rajpal, K. Cho, and A. G. Wilson (2023). Protein design with guided discrete diffusion. arXiv preprint arXiv:2305.20009. Guo, Y., H. Yuan, Y. Yang, M. Chen, and M. Wang (2024). Gradient guidance for diffusion models: An optimization perspective. arXiv preprint arXiv:2404.14743. Haarnoja, T., H. Tang, P. Abbeel, and S. Levine (2017). Reinforcement learning with deep energy- based policies. In International conference on machine learning, pp. 1352–1361. PMLR. Han, S., I. Shenfeld, A. Srivastava, Y. Kim, and P. Agrawal (2024). Value augmented sampling for language model alignment and personalization. arXiv preprint arXiv:2405.06639. Hayes, T., R. Rao, H. Akin, N. J. Sofroniew, D. Oktay, Z. Lin, R. Verkuil, V. Q. Tran, J. Deaton, M. Wiggert, et al. (2024). Simulating 500 million years of evolution with a language model. bioRxiv, 2024–07. Ho, J., A. Jain, and P. Abbeel (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems 33, 6840–6851. Ho, J. and T. Salimans (2022). Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598. Ho, J., T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet (2022). Video diffusion models. Advances in Neural Information Processing Systems 35, 8633–8646. 12 \n",
      "\n",
      "Page 13 Text:\n",
      "Irwin, J. J. and B. K. Shoichet (2005). ZINC- a free database of commercially available compounds for virtual screening. Journal of chemical information and modeling 45(1), 177–182. Jin, W., R. Barzilay, and T. Jaakkola (2018). Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pp. 2323–2332. PMLR. Jo, J., S. Lee, and S. J. Hwang (2022). Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning, pp. 10362– 10383. PMLR. Kitagawa, G. (1993). A monte carlo filtering and smoothing method for non-gaussian nonlinear state space models. In Proceedings of the 2nd US-Japan joint seminar on statistical time series analysis, Volume 110. Lal, A., D. Garfield, T. Biancalani, and G. Eraslan (2024). reglm: Designing realistic regulatory dna with autoregressive language models. bioRxiv, 2024–02. Leblond, R., J.-B. Alayrac, L. Sifre, M. Pislar, J.-B. Lespiau, I. Antonoglou, K. Simonyan, and O. Vinyals (2021). Machine translation decoding beyond beam search. arXiv preprint arXiv:2104.05336. Lee, S., J. Jo, and S. J. Hwang (2023). Exploring chemical space with score-based out-of-distribution generation. In International Conference on Machine Learning, pp. 18872–18892. PMLR. Levine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909. Lew, A. K., T. Zhi-Xuan, G. Grand, and V. K. Mansinghka (2023). Sequential monte carlo steering of large language models using probabilistic programs. arXiv preprint arXiv:2306.03081. Lou, A., C. Meng, and S. Ermon (2023). Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834. Mudgal, S., J. Lee, H. Ganapathy, Y. Li, T. Wang, Y. Huang, Z. Chen, H.-T. Cheng, M. Collins, T. Strohman, et al. (2023). Controlled decoding from language models. arXiv preprint arXiv:2310.17022. Nakano, R., J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. (2021). Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. Nisonoff, H., J. Xiong, S. Allenspach, and J. Listgarten (2024). Unlocking guidance for discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572. Prabhudesai, M., A. Goyal, D. Pathak, and K. Fragkiadaki (2023). Aligning text-to-image diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739. Qin, L., S. Welleck, D. Khashabi, and Y. Choi (2022). Cold decoding: Energy-based constrained text generation with langevin dynamics. Advances in Neural Information Processing Systems 35, 9538–9551. Sahoo, S. S., M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin, J. T. Chiu, A. Rush, and V. Kuleshov (2024). Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524. Sample, P. J., B. Wang, D. W. Reid, V. Presnyak, I. J. McFadyen, D. R. Morris, and G. Seelig (2019). Human 5’utr design and variant effect prediction from a massively parallel translation assay. Nature biotechnology 37(7), 803–809. Sarkar, A., Z. Tang, C. Zhao, and P. Koo (2024). Designing dna with tunable regulatory activity using discrete diffusion. bioRxiv, 2024–05. Schuhmann, C. (2022, Aug). LAION aesthetics. Shi, J., K. Han, Z. Wang, A. Doucet, and M. K. Titsias (2024). Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329. 13 \n",
      "\n",
      "Page 14 Text:\n",
      "Shi, Y., V. De Bortoli, A. Campbell, and A. Doucet (2024). Diffusion schr¨ odinger bridge matching. Advances in Neural Information Processing Systems 36. Sohl-Dickstein, J., E. Weiss, N. Maheswaranathan, and S. Ganguli (2015). Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 2256–2265. PMLR. Song, J., C. Meng, and S. Ermon (2020). Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Song, Y., J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole (2020). Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456. Stark, H., B. Jing, C. Wang, G. Corso, B. Berger, R. Barzilay, and T. Jaakkola (2024). Dirichlet flow matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841. Stiennon, N., L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020). Learning to summarize with human feedback. Advances in Neural Information Processing Systems 33, 3008–3021. Taskiran, I. I., K. I. Spanier, H. Dickm¨ anken, N. Kempynck, A. Panˇ c´ ıkov´ a, E. C. Eks ¸i, G. Hulselmans, J. N. Ismail, K. Theunis, R. Vandepoel, et al. (2024). Cell-type-directed design of synthetic enhancers. Nature 626(7997), 212–220. Touvron, H., L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Trott, O. and A. J. Olson (2010). Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry 31(2), 455–461. Uehara, M., Y. Zhao, T. Biancalani, and S. Levine (2024). Understanding reinforcement learning- based fine-tuning of diffusion models: A tutorial and review. arXiv preprint arXiv:2407.13734. Uehara, M., Y. Zhao, K. Black, E. Hajiramezanali, G. Scalia, N. L. Diamant, A. M. Tseng, T. Bian- calani, and S. Levine (2024). Fine-tuning of continuous-time diffusion models as entropy- regularized control. arXiv preprint arXiv:2402.15194. Uehara, M., Y. Zhao, E. Hajiramezanali, G. Scalia, G. Eraslan, A. Lal, S. Levine, and T. Biancalani (2024). Bridging model-based optimization and generative modeling via conservative fine-tuning of diffusion models. arXiv preprint arXiv:2405.19673. van Westen, G. J., R. F. Swier, I. Cortes-Ciriano, J. K. Wegner, J. P. Overington, A. P. IJzerman, H. W. van Vlijmen, and A. Bender (2013). Benchmarking of protein descriptor sets in proteochemo- metric modeling (part 2): modeling performance of 13 amino acid descriptor sets. Journal of cheminformatics 5, 1–20. Vignac, C., I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard (2022). Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734. Wang, Y., J. Yu, and J. Zhang (2022). Zero-shot image restoration using denoising diffusion null-space model. arXiv preprint arXiv:2212.00490. Watson, J. L., D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, et al. (2023). De novo design of protein structure and function with rfdiffusion. Nature 620(7976), 1089–1100. Wu, Y., M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al. (2016). Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Xu, K., W. Hu, J. Leskovec, and S. Jegelka (2018). How powerful are graph neural networks? arXiv preprint arXiv:1810.00826. 14 \n",
      "\n",
      "Page 15 Text:\n",
      "Xu, M., A. S. Powers, R. O. Dror, S. Ermon, and J. Leskovec (2023). Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pp. 38592– 38610. PMLR. Yang, K. and D. Klein (2021). Fudge: Controlled text generation with future discriminators. arXiv preprint arXiv:2104.05218. Yu, J., Y. Wang, C. Zhao, B. Ghanem, and J. Zhang (2023). Freedom: Training-free energy-guided conditional diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23174–23184. Zhao, S., R. Brekelmans, A. Makhzani, and R. Grosse (2024). Probabilistic inference in language models via twisted sequential monte carlo. arXiv preprint arXiv:2404.17546. Zhao, Y., M. Uehara, G. Scalia, T. Biancalani, S. Levine, and E. Hajiramezanali (2024). Adding con- ditional control to diffusion models with reinforcement learning. arXiv preprint arXiv:2406.12120. 15 \n",
      "\n",
      "Page 16 Text:\n",
      "Algorithm 4 Decoding via Sequential Monte Carlo in Diffusion Models 1: Require: Estimated value functions {ˆ vt(x)}0 t=T , pre-trained diffusion models {ppre t }0 t=T , hyper- parameter α ∈R, Batch size N 2: for t ∈[T + 1, · · · , 0] do 3: IS step: 4: i ∈[1, · · · , N]; x[i] t−1 ∼ppre t−1(·|x[i] t ), w[i] t−1 := exp(ˆ vt−1(x[i] t−1)/α) exp(ˆ vt(x[i] t )/α) 5: Selection step: select new indices with replacement 6: {x[i] t−1}N i=1 ←{x ζ[i] t−1 t−1 }N i=1, {ζ[i] t−1}N i=1 ∼Cat \u001a w[i] t−1 PN j=1 w[j] t−1 \u001bN i=1 ! 7: end for 8: Output: x0 A Soft Value-Based Decoding with Particle Filter In this section, we explain soft-value decoding incorporating particle filtering (Doucet et al., 2009). However, we generally do not recommend this approach in practice due to its difficulty in paralleliza- tion. The complete algorithm is summarized in Algorithm 4. Here, we provide a brief overview. It consists of two steps. Since our algorithm is iterative, at time point t, consider we have N samples (particles) {x[i] t }i = 1N. IS step (line 3). We generate a set of samples {x[i] t−1}N i=1 following a policy from a pre-trained model ppre t−1(·|·). In other words, ∀i ∈[1, · · · , N]; x[i] t−1 ∼ppre t−1(·|x[i] t ). Now, we denote the importance weight for the next particle xt−1 given the current particle xt as w(xt−1, xt), expressed as w(xt−1, xt) := exp(vt−1(xt−1)/α) R exp(vt−1(xt−1)/α)ppre t−1(xt−1|xt)dxt−1 = exp(vt−1(xt−1)/α) exp(vt(xt)/α) , and define ∀i ∈[1, · · · , N]; w[i] t−1 := w(x[i] t−1, x[i] t ). Note here we have used the soft Bellman equation: exp(vt(xt)/α) = Z exp(vt−1(xt−1)/α)ppre t−1(xt−1|xt)dxt−1. Hence, by denoting the target marginal distribution at t −1, we have the following approximation: ptar t−1 ≈ |{z} IS N X i=1 w[i] t−1 PN j=1 w[j] t−1 δx[i] t−1. Selection step (line 5). Finally, we consider a resampling step. The resampling indices are determined by the following: {ζ[i] t−1}N i=1 ∼Cat   ( w[i] t−1 PN j=1 w[j] t−1 )N i=1  . To summarize, we conduct ptar t−1 ≈ |{z} IS N X i=1 w[i] t−1 PN j=1 w[j] t−1 δx[i] t−1 ≈ |{z} Resampling 1 N N X i=1 δ x ζ[i] t−1 t−1 . Remark 4 (Works in autoregressive models). We note that in the context of autoregressive (language) models, Zhao et al. (2024); Lew et al. (2023) proposed a similar algorithm. 16 \n",
      "\n",
      "Page 17 Text:\n",
      "Algorithm 5 Value Function Estimation Using Soft Q-learning 1: Require: Pre-trained diffusion models {ppre t }0 t=T , value function model v(x; θ) 2: Collect datasets {x(s) T , · · · , x(s) 0 }S s=1 by rolling-out {ppre t }0 t=T from t = T to t = 0. 3: for j ∈[0, · · · , J] do 4: Update θ by running regression: θ′ j ←argmin θ T X t=0 S X s=1 n v(x(s) t ; θ) −v(x(s) t−1; θ′ j−1) o2 . 5: end for 6: Output: v(x; θ′ J) B Soft Q-learning In this section, we explain soft value iteration to estimate soft value functions, which serves as an alternative to Monte Carlo regression. Soft Bellman equation. Here, we use the soft Bellman equation: exp(vt(xt)/α) = Z exp(vt−1(xt−1)/α)ppre t−1(xt−1|xt)dxt−1, as proved in Section 4.1 in (Uehara et al., 2024). In other words, vt(xt) = α log{Ext−1∼ppre(·|xt) [exp(vt−1(xt−1)/α)|xt]}. Algorithm. Based on the above, we can estimate soft value functions recursively by regressing vt−1(xt−1) onto xt. This approach is often referred to as soft Q-learning in the reinforcement learning literature (Haarnoja et al., 2017; Levine, 2018). In our context, due to the concern of scaling of α, as we have done in Algorithm 2, we had better use vt(xt) = Ext−1∼ppre(·|xt) [vt−1(xt−1)|xt] . With the above recursive equation, we can estimate soft value functions as in Algorithm 5. C Additional Experimental Details We further add additional experimental details. C.1 Additional Setups for Experiments Images. • DPS: We require differentiable models that map images to compressibility. For this task, we have used a standard CNN. Molecules. • DPS: Following the implementation in Lee et al. (2023), we use the same GNN model as the reward model. Note that this model cannot compute derivatives with respect to adjacency matrices. • SVDD-MC: We use a Graph Isomorphism Network (GIN) model (Xu et al., 2018) as a value function model. Enhancers. • DPS: Although DPS was originally proposed in the continuous space, we have adapted it for our use by incorporating the gradient of the value function model at each step and representing each sequence as a one-hot encoding vector. • SVDD-MC: We have used the Enformer model as the function model. 17 \n",
      "\n",
      "Page 18 Text:\n",
      "5’UTRs. We have used ConvGRU as the reward model (Dey and Salem, 2017). • DPS: Although DPS was originally proposed in the continuous space, we have adapted it for our use by incorporating the gradient of the value function model at each step and representing each sequence as a one-hot encoding vector. • SVDD-MC: We employed ConvGRU as the value function model. C.2 Additional Results We evaluate the performance of the generated samples using histograms in Section 6. Here, we present the top 10 and 20 quantiles of the generated samples in Table 2. Table 2: Top 10 and 20 quantiles of the generated samples for each algorithm. Higher is better. Domain Quantile Pre-Train Best-N DPS SVDD-MC SVDD-PM Images compress 20% -86.2 -63.2 -67.1 - -43.7 10% -78.6 -57.3 -61.2 - -38.8 Images aesthetic 20% 5.875 6.246 5.868 - 6.356 10% 5.984 6.343 5.997 - 6.472 Molecules QED 20% 0.771 0.881 0.802 0.912 0.916 10% 0.812 0.902 0.843 0.925 0.928 Molecules SA 20% 0.750 0.916 0.802 1.0 1.0 10% 0.803 0.941 0.849 1.0 1.0 Enhancers 20% 0.74 3.00 2.68 5.53 6.44 10% 1.41 3.52 3.85 5.75 7.02 5’UTR MRL 20% 0.78 0.97 0.90 1.09 1.33 10% 0.86 1.021 0.93 1.12 1.38 5’UTR Stability 20% -0.63 -0.59 -0.62 -0.52 -0.56 10% -0.61 -0.58 -0.60 -0.51 -0.55 Performance of value function training. We report the performance of value function learning using Monte Carlo regression as follows. We plot the Pearson correlation on the test dataset. (a) Images: compressibil- ity (b) Images: aesthetic score (c) Molecules: QED Figure 5: Training curve More generated samples. We have provided additional generated samples in Figure 6, Figure 7, Figure 8 and Figure 9. 18 \n",
      "\n",
      "Page 19 Text:\n",
      "-39 -20 -39 -31 -43 -37 -74 -61 -73 -68 -81 -79 Best-N SVDD-PM -112 -130 Pre-trained -103 -155 -97 -103 Figure 6: Additional generated samples (Domain: images, Reward: Compressibility) 6.5 6.5 6.6 6.9 6.5 6.5 6.0 5.8 5.9 5.7 6.1 6.1 Best-N SVDD-PM 5.5 5.6 Pre-trained 5.7 5.5 5.6 5.3 Figure 7: Additional generated samples (Domain: Images, Reward: Aesthetic score) 19 \n",
      "\n",
      "Page 20 Text:\n",
      "Figure 8: Additional generated samples (Domain: Molecules, Reward: QED score) Figure 9: Additional generated samples (Domain: Molecules, Reward: SA score, normalized as (10 −SA)/9) 20 \n",
      "\n",
      "Page 21 Text:\n",
      "Figure 10: Additional generated samples from SVDD(Domain: Molecules, Reward: SA score = 1.0 (normalized as (10 −SA)/9)) 21 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#extracting text page by page without spaces\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Load your PDF file\n",
    "#file_path = \n",
    "pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "# Initialize an empty string to store the entire text\n",
    "full_text = \"\"\n",
    "\n",
    "# Extract text from each page\n",
    "for page_num in range(len(pdf_document)):\n",
    "    page = pdf_document.load_page(page_num)  # Load the page\n",
    "\n",
    "    # Extract text\n",
    "    text = page.get_text()\n",
    "\n",
    "    # Remove spaces before each line and join the lines together\n",
    "    processed_text = ' '.join([line.strip() for line in text.split('\\n')])\n",
    "    \n",
    "    # Append the processed text to the full_text string\n",
    "    full_text += f'Page {page_num + 1} Text:\\n{processed_text}\\n\\n'\n",
    "\n",
    "    #print(f'Page {page_num + 1} Text:\\n{processed_text}\\n')\n",
    "\n",
    "# Print the entire text\n",
    "print(full_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a06d65",
   "metadata": {},
   "source": [
    "# Extract pagewise Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fa69290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import fitz\\nimport os\\nfrom PIL import Image\\nimport io\\nimport shutil\\n\\ndef clean_output_directory(output_dir):\\n    if os.path.exists(output_dir):\\n        shutil.rmtree(output_dir)\\n        print(f\"Removed existing output directory: {output_dir}\")\\n    os.makedirs(output_dir)\\n    print(f\"Created new output directory: {output_dir}\")\\n\\ndef extract_images_from_pdf(pdf_path, output_dir):\\n    # Clean the output directory\\n    clean_output_directory(output_dir)\\n\\n    # Open the PDF file\\n    pdf_document = fitz.open(pdf_path)\\n\\n    # Keep track of folders with images\\n    folders_with_images = set()\\n\\n    # Iterate through each page\\n    for page_num in range(len(pdf_document)):\\n        page = pdf_document[page_num]\\n        \\n        # Create a folder for the current page\\n        page_folder = os.path.join(output_dir, f\"page_{page_num + 1}\")\\n        os.makedirs(page_folder, exist_ok=True)\\n        \\n        # Get list of images on the page\\n        image_list = page.get_images()\\n        \\n        # Iterate through each image on the page\\n        for img_index, img in enumerate(image_list):\\n            # Get the image data\\n            xref = img[0]\\n            base_image = pdf_document.extract_image(xref)\\n            image_bytes = base_image[\"image\"]\\n            \\n            # Get the image extension\\n            image_ext = base_image[\"ext\"]\\n            \\n            # Create a PIL Image object\\n            image = Image.open(io.BytesIO(image_bytes))\\n            \\n            # Save the image\\n            image_filename = f\"image_{img_index + 1}.{image_ext}\"\\n            image_path = os.path.join(page_folder, image_filename)\\n            image.save(image_path)\\n            \\n            print(f\"Saved image: {image_path}\")\\n            \\n            # Mark this folder as containing images\\n            folders_with_images.add(page_folder)\\n\\n    # Close the PDF document\\n    pdf_document.close()\\n\\n    # Remove empty folders\\n    remove_empty_folders(output_dir, folders_with_images)\\n\\ndef remove_empty_folders(output_dir, folders_with_images):\\n    for root, dirs, files in os.walk(output_dir, topdown=False):\\n        for dir_name in dirs:\\n            dir_path = os.path.join(root, dir_name)\\n            if dir_path not in folders_with_images and not os.listdir(dir_path):\\n                os.rmdir(dir_path)\\n                print(f\"Removed empty folder: {dir_path}\")\\n\\n# Usage example\\npdf_path = \"4.pdf\"\\noutput_directory = \"extracted_images\"\\n\\nextract_images_from_pdf(pdf_path, output_directory)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import fitz\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import shutil\n",
    "\n",
    "def clean_output_directory(output_dir):\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "        print(f\"Removed existing output directory: {output_dir}\")\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created new output directory: {output_dir}\")\n",
    "\n",
    "def extract_images_from_pdf(pdf_path, output_dir):\n",
    "    # Clean the output directory\n",
    "    clean_output_directory(output_dir)\n",
    "\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # Keep track of folders with images\n",
    "    folders_with_images = set()\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document[page_num]\n",
    "        \n",
    "        # Create a folder for the current page\n",
    "        page_folder = os.path.join(output_dir, f\"page_{page_num + 1}\")\n",
    "        os.makedirs(page_folder, exist_ok=True)\n",
    "        \n",
    "        # Get list of images on the page\n",
    "        image_list = page.get_images()\n",
    "        \n",
    "        # Iterate through each image on the page\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            # Get the image data\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Get the image extension\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            \n",
    "            # Create a PIL Image object\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            \n",
    "            # Save the image\n",
    "            image_filename = f\"image_{img_index + 1}.{image_ext}\"\n",
    "            image_path = os.path.join(page_folder, image_filename)\n",
    "            image.save(image_path)\n",
    "            \n",
    "            print(f\"Saved image: {image_path}\")\n",
    "            \n",
    "            # Mark this folder as containing images\n",
    "            folders_with_images.add(page_folder)\n",
    "\n",
    "    # Close the PDF document\n",
    "    pdf_document.close()\n",
    "\n",
    "    # Remove empty folders\n",
    "    remove_empty_folders(output_dir, folders_with_images)\n",
    "\n",
    "def remove_empty_folders(output_dir, folders_with_images):\n",
    "    for root, dirs, files in os.walk(output_dir, topdown=False):\n",
    "        for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            if dir_path not in folders_with_images and not os.listdir(dir_path):\n",
    "                os.rmdir(dir_path)\n",
    "                print(f\"Removed empty folder: {dir_path}\")\n",
    "\n",
    "# Usage example\n",
    "pdf_path = \"4.pdf\"\n",
    "output_directory = \"extracted_images\"\n",
    "\n",
    "extract_images_from_pdf(pdf_path, output_directory)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d82cb",
   "metadata": {},
   "source": [
    "## Section wise image extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdbb9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing output directory: extracted_images\n",
      "Created new output directory: extracted_images\n",
      "Saved image: extracted_images\\Methodology\\image_1.png\n",
      "Saved image: extracted_images\\Methodology\\image_2.png\n",
      "Saved image: extracted_images\\Methodology\\image_3.png\n",
      "Saved image: extracted_images\\Methodology\\image_4.png\n",
      "Saved image: extracted_images\\Methodology\\image_5.png\n",
      "Saved image: extracted_images\\Methodology\\image_6.png\n",
      "Saved image: extracted_images\\Methodology\\image_7.png\n",
      "Saved image: extracted_images\\Methodology\\image_8.png\n",
      "Saved image: extracted_images\\Methodology\\image_9.png\n",
      "Saved image: extracted_images\\Methodology\\image_10.png\n",
      "Saved image: extracted_images\\Methodology\\image_11.png\n",
      "Saved image: extracted_images\\Methodology\\image_12.png\n",
      "Saved image: extracted_images\\Methodology\\image_13.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_14.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_15.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_16.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_17.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_18.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_19.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_20.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_21.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_22.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_23.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_24.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_25.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_26.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_27.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_28.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_29.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_30.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_31.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_32.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_33.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_34.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_35.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_36.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_37.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_38.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_39.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_40.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_41.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_42.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_43.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_44.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_45.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_46.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_47.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_48.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_49.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_50.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_51.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_52.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_53.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_54.png\n",
      "Saved image: extracted_images\\Results and Discussion\\image_55.png\n",
      "Removed empty folder: extracted_images\\Introduction\n",
      "Removed empty folder: extracted_images\\Other\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "def clean_output_directory(output_dir):\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "        print(f\"Removed existing output directory: {output_dir}\")\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created new output directory: {output_dir}\")\n",
    "\n",
    "def determine_section(text):\n",
    "    text = text.lower()\n",
    "    if \"introduction\" in text or \"background\" in text:\n",
    "        return \"Introduction\"\n",
    "    elif \"method\" in text or \"methodology\" in text or \"experimental\" in text:\n",
    "        return \"Methodology\"\n",
    "    elif \"result\" in text or \"discussion\" in text or \"conclusion\" in text:\n",
    "        return \"Results and Discussion\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "def extract_images_from_pdf(pdf_path, output_dir):\n",
    "    clean_output_directory(output_dir)\n",
    "\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    sections = [\"Introduction\", \"Methodology\", \"Results and Discussion\", \"Other\"]\n",
    "    for section in sections:\n",
    "        os.makedirs(os.path.join(output_dir, section), exist_ok=True)\n",
    "\n",
    "    current_section = \"Other\"\n",
    "    image_counter = 1\n",
    "\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document[page_num]\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Check for section headers\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            new_section = determine_section(line)\n",
    "            if new_section != \"Other\":\n",
    "                current_section = new_section\n",
    "                break\n",
    "\n",
    "        image_list = page.get_images()\n",
    "        \n",
    "        for img in image_list:\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            \n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            \n",
    "            image_filename = f\"image_{image_counter}.{image_ext}\"\n",
    "            image_path = os.path.join(output_dir, current_section, image_filename)\n",
    "            image.save(image_path)\n",
    "            \n",
    "            print(f\"Saved image: {image_path}\")\n",
    "            image_counter += 1\n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    # Remove empty folders\n",
    "    for root, dirs, files in os.walk(output_dir, topdown=False):\n",
    "        for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            if not os.listdir(dir_path):\n",
    "                os.rmdir(dir_path)\n",
    "                print(f\"Removed empty folder: {dir_path}\")\n",
    "\n",
    "# Usage example\n",
    "#pdf_path = \"4.pdf\"\n",
    "output_directory = \"extracted_images\"\n",
    "\n",
    "extract_images_from_pdf(pdf_path, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55919e35",
   "metadata": {},
   "source": [
    "Here are the key changes and additions to the script:\n",
    "A new function determine_section() has been added. This function tries to determine which section a given text belongs to based on keywords.\n",
    "In the extract_images_from_pdf() function:\n",
    "We create folders for each section at the start.\n",
    "We initialize current_section as \"Other\" and keep track of the current section as we process the PDF.\n",
    "For each page, we extract the text and check for section headers using the determine_section() function.\n",
    "Images are saved in the folder corresponding to the current section.\n",
    "We use a global image_counter to ensure unique image names across all sections.\n",
    "The script still cleans the output directory at the start and removes empty folders at the end.\n",
    "This approach has some limitations:\n",
    "It relies on finding specific keywords in the text to determine sections. This may not work perfectly for all papers, especially if they use unconventional section names.\n",
    "It assumes that once a section is detected, all following images belong to that section until a new section is found. This might not always be accurate.\n",
    "Some images might end up in the \"Other\" folder if they appear before any recognizable section header."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d073227",
   "metadata": {},
   "source": [
    "## Trying to use LLM to summarize the individual sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2e123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Text:\n",
      "Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding Xiner Li 1 Yulai Zhao 2 Chenyu Wang 3 Gabriele Scalia 4 Gokcen Eraslan4 Surag Nair4 Tommaso Biancalani 4 Aviv Regev 4∗ Sergey Levine5∗ Masatoshi Uehara 4∗ 1Texas A&M University 2 Princeton University 3 MIT 4 Genentech 5 UC Berkeley Abstract Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely -generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require “differentiable” proxy models (e.g., classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (e.g., classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation. The code is available at https://github.com/masa-ue/SVDD. 1 Introduction Diffusion models have gained popularity as powerful generative models. Their applications extend beyond image generation to include natural language generation (Sahoo et al., 2024; Shi et al., 2024; Lou et al., 2023), molecule generation (Jo et al., 2022; Vignac et al., 2022), and biological (DNA, RNA, protein) sequence generation (Avdeyev et al., 2023; Stark et al., 2024), and in each of these domains diffusion models have been shown to be very effective at capturing complex natural distributions. However, in practice, we might not only want to generate realistic samples, but to produce samples that optimize specific downstream reward functions while preserving naturalness by leveraging pre-trained models. For example, in computer vision, we might aim to generate natural images with high aesthetic and alignment scores. In drug discovery, we may seek to generate valid molecules with high QED/SA/docking scores (Lee et al., 2023; Jin et al., 2018) or natural RNAs (such as mRNA vaccines (Cheng et al., 2023)) with high translational efficiency and stability (Castillo-Hair and Seelig, 2021; Asrani et al., 2018), and natural DNAs with high cell-specificity (Gosai et al., 2023; Taskiran et al., 2024; Lal et al., 2024). The optimization of downstream reward functions using pre-trained diffusion models has been approached in various ways. In our work, we focus on non-fine-tuning-based methods because fine- tuning generative models (e.g., when using classifier-free guidance (Ho et al., 2020) or RL-based fine- ∗regev.aviv@gene.com, svlevine@eecs.berkeley.edu , uehara.masatoshi@gene.com Preprint. Under review. arXiv:2408.08252v1  [cs.LG]  15 Aug 2024 \n",
      "\n",
      "Page 2 Text:\n",
      "Table 1: A comparison of our method with a number of prior approaches. “Non-differentiable” refers to the method’s capability to operate without requiring differentiable proxy models. “No Training” means that no additional training of the diffusion model is required as long as we have access to the reward feedback. No fine-tuning Non-differentiable No Training Classifier guidance (Dhariwal and Nichol, 2021) ✓ DPS (Chung et al., 2022) ✓ ✓ Classifier-free (Ho and Salimans, 2022) ✓ RL fine-tuning ✓ SVDD-MC (Ours) ✓ ✓ SVDD-PM (Ours) ✓ ✓ ✓ tuning (Black et al., 2023; Fan et al., 2023; Uehara et al., 2024; Clark et al., 2023; Prabhudesai et al., 2023)) often becomes computationally intensive, especially as pre-trained generative models grow larger in the era of “foundation models”. Although classifier guidance and its variants (e.g., Dhariwal and Nichol (2021); Song et al. (2020); Chung et al. (2022); Bansal et al. (2023); Ho et al. (2022)) have shown success as non-fine-tuning methods in these settings, they face significant challenges. Firstly, as they would require constructing differentiable proxy models, they cannot directly incorporate useful domain-specific non-differentiable features (e.g., molecular/protein descriptors (van Westen et al., 2013; Ghiringhelli et al., 2015; Emonts and Buyel, 2023)) or non-differentiable reward feedback (e.g., physics-based simulations such as Vina and Rosetta (Trott and Olson, 2010; Alhossary et al., 2015; Alford et al., 2017)). This limitation also hinders the application of current classifier guidance methods to recently developed discrete diffusion models (Austin et al., 2021; Campbell et al., 2022; Sahoo et al., 2024; Shi et al., 2024; Lou et al., 2023) in a principled manner (i.e., without transforming the discrete space into the Euclidean space). Additionally, gradient computation can be memory-intensive with large proxy models. A C C G U A M C G M A M M G M xt xt−1 xT x0 v(xt−1) := E[r(x0)|xt−1] M M M M M A M M G M A M G M A M M G M C A M G M U 2.4 4.2 3.5 RNA Translational efficiency r(x0) Figure 1: Summary of SVDD. The notation v denotes value functions that predict reward r(x0) (at time 0) from states at time t −1. SVDD involves two steps: (1) generating multiple noisy states from pre-trained models and (2) selecting the state with the highest value according to the value function. We propose a novel method, SVDD (Soft Value- based Decoding in Diffusion models), for op- timizing downstream reward functions in diffu- sion models (summarized in Figure 1) to address the aforementioned challenges. Inspired by re- cent literature on RL-based fine-tuning (Uehara et al., 2024), we first introduce soft value func- tions that serve as look-ahead functions, indicat- ing how intermediate noisy samples lead to high rewards in the future of the diffusion denoising process. After learning (or approximating) these value functions, we present a new inference-time technique, SVDD, which obtains multiple noisy states from the policy (i.e., denoising map) of pre- trained diffusion models and selects the sample with the highest value function at each time step. Specifically, we introduce two algorithms (SVDD- MC and SVDD-PM) depending on how we es- timate value functions. Notably, the SVDD-PM approach, leveraging the characteristics of diffu- sion models (e.g., utilizing the forward process in diffusion models to directly map t to 0 in terms of expectation in Figure 1), requires no additional training as long as we have access to the reward feedback. Our contributions are summarized as follows (also see Table 1). We propose a novel technique for optimizing downstream reward functions in pre-trained diffusion models that eliminates the need to construct differentiable proxy models and avoids the need to fine-tune the generative model itself. The first property allows for the use of non-differentiable reward feedback, which is common in many scientific fields, and makes our method applicable to recent discrete diffusion models without any modification. The second property addresses the high computational cost associated with fine-tuning diffusion models. We demonstrate the effectiveness of our methods across various domains, including image generation, molecule generation, and DNA/RNA generation. 2 \n",
      "\n",
      "Page 3 Text:\n",
      "2 Related Works Here, we summarize related work. We first outline methods relevant to our goal, categorizing them based on whether they involve fine-tuning. We then discuss related directions, such as discrete diffusion models, where our method excels, and decoding in autoregressive models. Non-fine-tuning methods. There are primarily two methods for optimizing downstream functions in diffusion models without fine-tuning. • Classifier guidance (Dhariwal and Nichol, 2021; Song et al., 2020): It has been widely used to condition pre-trained diffusion models without fine-tuning. Although these methods do not originally focus on optimizing reward functions, they can be applied for this purpose (Uehara et al., 2024, Section 6.2). In this approach, an additional derivative of a certain value function is incorporated into the drift term (mean) of pre-trained diffusion models during inference. Subsequent variants (e.g., Chung et al. (2022); Ho et al. (2022); Bansal et al. (2023); Guo et al. (2024); Wang et al. (2022); Yu et al. (2023)) have been proposed to simplify the learning of value functions. However, these methods require constructing differentiable models, which limits their applicability to non-differentiable features/reward feedbacks commonly encountered in scientific domains as mentioned in Section 1. Additionally, this approach cannot be directly extended to discrete diffusion models in a principle way 2. Our approach aims to address these challenges. • Best-of-N: The naive way is to generate multiple samples and select the top samples based on the reward functions, known as best-of-K in the literature on (autoregressive) LLMs (Stiennon et al., 2020; Nakano et al., 2021; Touvron et al., 2023; Beirami et al., 2024; Gao et al., 2023). This approach is significantly less efficient than ours, as our method uses soft-value functions that predict how intermediate noisy samples lead to high rewards in the future. We validate this experimentally in Section 6. Fine-tuning of diffusion models. Several methods exist for fine-tuning generative models to optimize downstream reward functions, such as classifier-free guidance (Ho and Salimans, 2022) and RL-based fine-tuning (Fan et al., 2023; Black et al., 2023)/its variants (Dong et al., 2023). However, these approaches often come with caveats, including high computational costs and the risk of easily forgetting pre-trained models. In our work, we propose an inference-time technique that eliminates the need for fine-tuning generative models, similar to classifier guidance. Discrete diffusion models. Based on seminal works Austin et al. (2021); Campbell et al. (2022), recent work on masked diffusion models (Lou et al., 2023; Shi et al., 2024; Sahoo et al., 2024) has demonstrated their strong performance in natural language generation. Additionally, they have been applied to biological sequence generation (e.g., DNA, protein sequences in Campbell et al. (2024); Sarkar et al. (2024)). In these cases, the use of diffusion models over autoregressive models is particularly apt, given that many biological sequences ultimately adopt complex three-dimensional structures. We also note that ESM3 (Hayes et al., 2024), a widely recognized foundational model in protein sequence generation, bears similarities to masked diffusion models. Despite its significance, it cannot be integrated with standard classifier guidance because adding a continuous gradient to a discrete objective is not inherently valid. Unlike standard classifier guidance, our algorithm can be seamlessly applied to discrete diffusion models. Decoding in autoregressive models with rewards. The decoding strategy, which dictates how sentences are generated from the model, is a critical component of text generation in autoregressive language models (Wu et al., 2016; Chorowski and Jaitly, 2016; Leblond et al., 2021). Recent studies have explored inference-time techniques for optimizing downstream reward functions Dathathri et al. (2019); Yang and Klein (2021); Qin et al. (2022); Mudgal et al. (2023); Zhao et al. (2024); Han et al. (2024). While there are similarities between these works and ours, to the best of our knowledge, no prior work has extended such methodologies to diffusion models. Furthermore, our approach leverages characteristics unique to diffusion models that are not present in autoregressive models such as SVDD-PM. 2We note that a notable exception has been recently proposed (Nisonoff et al., 2024). However, our method can be applied to both continuous and discrete diffusion models in a unified manner. 3 \n",
      "\n",
      "Page 4 Text:\n",
      "3 Preliminaries and Goal In this section, we describe the standard method for training diffusion models and outline the objective of our work: optimizing downstream reward functions given pre-trained diffusion models. 3.1 Diffusion Models In diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020), our goal is to learn a sampler p(x|c) ∈[C →∆(X)] given data consisting of (x, c) ∈X × C. For instance, in text-to-image diffusion models, c is a text, and x is an image. In protein inverse folding, c is a backbone protein structure, and x is a protein sequence. The training process for a standard diffusion model is summarized as follows. First, we introduce a (fixed) forward process qt : X →∆(X). Now, after introducing the forward process, we aim to learn a backward process: {pt} where each pt is X × C →∆(X) so that the induced distributions induced by the forward process and backward process match marginally. For this purpose, by parametrizing the backward processes with θ ∈Rd, we typically use the following loss function: Ex1,··· ,xT ∼q(·|x0) \" −log p0(x0|x1) + T X t=1 KL(qt(· | xt−1)∥pt(· | xt+1, c; θ)) + KL(qT (·)∥pT (·)) # , which is derived from the variational lower bound of the negative likelihood (i.e., ELBO). Here are two examples of concrete parameterizations. Let αt ∈R be a noise schedule. Example 1 (Continuous space). When X is Euclidean space, we typically use the Gaussian distribu- tion qt(· | x) = N(√αtx, (1 −αt)). Then, the backward process is parameterized as N \u0012√αt(1 −¯ αt+1)xt + √αt−1(1 −αt)ˆ x0(xt, c; θ) 1 −¯ αt , (1 −αt)(1 −¯ αt−1) 1 −¯ αt \u0013 , where ¯ αt = Qt i=1 αi. Here, ˆ x0(xt, c; θ) is a neural network that predicts x0 from xt (i.e., Eq[x0|xt, c]). Example 2 (Discrete space in Sahoo et al. (2024); Shi et al. (2024)). Let X be a space of one-hot column vectors {x ∈{0, 1}K : PK i=1 xi = 1}, and Cat(π) be the categorical distribution over K classes with probabilities given by π ∈∆K where ∆K denotes the K-simplex. A typical choice is qt(· | x) = Cat(αtx + (1 −αt)m) where m = [0, · · · , 0, Mask]. Then, the backward process is parameterized as ( Cat(xt), if xt ̸= m Cat \u0010 (1−αt−1)m+(αt−1−αt)ˆ x0(xt,c;θ) 1−αt \u0011 , if xt = m. Here, ˆ x0(xt, c; θ) is a neural network that predicts x0 from xt. Note that when considering a sequence of L tokens (x1:L), we use the direct product: pt(x1:L t |x1:L t+1, c) = QL l=1 pt(xl t|x1:L t+1, c). After learning the backward process, we can sample from a distribution that emulates training data distribution (i.e., p(x|c)) by sequentially sampling {pt}0 t=T from t = T to t = 0. Notation. The notation δa denotes a Dirac delta distribution centered at a. The notation ∝indicates that the distribution is equal up to a normalizing constant. With slight abuse of notation, we often denote pT (·|·, ·) by pT (·). 3.2 Objective: Generating Samples with High Rewards While Preserving Naturalness We consider a scenario where we have a pre-trained diffusion model, which is trained using the loss function explained in Section 3.1. These pre-trained models are typically designed to excel at characterizing the natural design space (e.g., image space, biological space, or chemical space) by emulating the extensive training dataset. Our work focuses on obtaining samples that also optimize downstream reward functions r : X →R (e.g., QED and SA in molecule generation) while maintaining the naturalness by leveraging pre-trained diffusion models. We formalize this goal as follows. 4 \n",
      "\n",
      "Page 5 Text:\n",
      "Given a pre-trained model {ppre t }0 t=T , we denote the induced distribution by ppre(x|c) (i.e., R {Q1 t=T +1 ppre t−1(xt−1|xt, c)}dx1:T ). We aim to sample from the following distribution: p(α)(x|c) := argmax p∈[C→∆(X)] Ex∼p(·|c)[r(x)] | {z } term (a) −α KL(p(· | c)∥ppre(·|c)) | {z } term(b) ∝exp(r(x)/α)ppre(x|c). Here, the term (a) is introduced to optimize reward function, while the term (b) is used to maintain the naturalness of the generated samples. Existing methods. Several existing approaches target this goal (or its variant), as discussed in Section 2, including classifier guidance, fine-tuning (RL-based or classifier-free), and Best-of-N. In our work, we focus on non-fine-tuning-based methods, such as classifier guidance and Best-of-N. Specifically, we aim to address the limitations of these methods: the requirement for differentiable proxy models in classifier guidance and the inefficiency of Best-of-N. 4 Soft Value-Based Decoding in Diffusion Models In this section, we first present the motivation behind developing our new algorithm. We then introduce our algorithm, which satisfies desired properties, i.e., the lack of need for fine-tuning or constructing differentiable models. 4.1 Key Observation We introduce several key concepts. First, for t ∈[T + 1, · · · , 1], we define the soft value function: vt−1(·) := α log Ex0∼ppre(x0|xt−1) \u0014 exp \u0012r(x0) α \u0013 |xt−1 = · \u0015 , where E{ppre}[·] is induced by {ppre t (·|xt+1, c)}0 t=T . This value function represents the expected future reward at t = 0 from the intermediate noisy state at t −1. Next, we define the following soft optimal policy (denoising process) p⋆,α t−1 : X × C →∆(X) weighted by value functions vt−1 : X →R: p⋆,α t−1(·|xt, c) = ppre t−1(·|xt, c) exp(vt−1(·)/α) R ppre t−1(x|xt, c) exp(vt−1(x)/α)dx. Here, we refer to vt as soft value functions, p⋆,α t as soft optimal policies, respectively, because they literally correspond to soft value functions and soft optimal policies where we embed diffusion models into entropy-regularized MDPs, as demonstrated in Uehara et al. (2024). With this preparation in mind, we utilize the following key observation: Theorem 1 (From Theorem 1 in Uehara et al. (2024)). The distribution induced by {p⋆,α t (·|xt+1, c)}0 t=T is the target distribution p(α)(x|c), i.e., p(α)(x0|c) = R nQ1 t=T +1 p⋆,α t−1(xt−1|xt, c) o dx1:T . While Uehara et al. (2024) presents this theorem, they use it primarily to interpret their RL-based fine-tuning introduced in Fan et al. (2023); Black et al. (2023). In contrast, our work explores how to convert this into a new fine-tuning-free optimization algorithm. Our motivation for a new algorithm. Theorem 1 states that if we can hypothetically sample from {p⋆,α t }0 t=T , we can sample from the target distribution p(α). However, there are two challenges in sampling from each p⋆,α t−1: 1. The soft-value function vt−1 in p⋆,α t−1 is unknown. 2. It is unnormalized (i.e., calculating the normalizing constant is often hard). 5 \n",
      "\n",
      "Page 6 Text:\n",
      "We address the first challenge later in Section 4.3. Assuming the first challenge is resolved, we consider how to tackle the second challenge. A natural approach is to use importance sampling (IS): p⋆,α t−1(·|xt, c) ≈ M X m=1 w⟨m⟩ t−1 P j w⟨j⟩ t−1 δx⟨m⟩ t−1, {x⟨m⟩ t−1}M m=1 ∼ppre t−1(· | xt, c), where w⟨m⟩ t−1 := exp(vt(x⟨m⟩ t−1)/α). Thus, we can approximately sample from p⋆,α t−1(·|xt, c) by obtaining multiple (M) samples from pre-trained diffusion models and selecting the sample based on an index, which is determined by sampling from the categorical distribution with mean {w⟨m⟩ t−1/ P j w⟨j⟩ t−1}⟨M⟩ m=1. Difference over Best-of-N. Best-of-N, which generates multiple samples and selects the highest reward sample, is technically considered IS where the proposal distribution is the entire ppre(x0|c) = R Q t{ppre t (xt−1 | xt, c)}dx1:T . However, the use of importance sampling in our algorithm differs significantly, as we apply it at each time step to approximate each soft-optimal policy. 4.2 Inference-Time Algorithm Algorithm 1 SVDD (Soft Value-Based Decoding in Diffusion Models) 1: Require: Estimated soft value function {ˆ vt}0 t=T (refer to Algorithm 2 or Algorithm 3), pre- trained diffusion models {ppre t }0 t=T , hyperparameter α ∈R 2: for t ∈[T + 1, · · · , 1] do 3: Get M samples from pre-trained polices {x⟨m⟩ t−1}M m=1 ∼ppre t−1(·|xt), and for each m, calculate w⟨m⟩ t−1 := exp(ˆ vt−1(x⟨m⟩ t−1)/α) 4: xt−1 ←x⟨ζt−1⟩ t−1 after selecting an index: ζt−1 ∼Cat \u001a w⟨m⟩ t−1 PM j=1 w⟨j⟩ t−1 \u001bM m=1 ! , 5: end for 6: Output: x0 Now, we introduce our algorithm, which leverages the observation, in Algorithm 1. Our algorithm is an iterative sampling method that integrates soft value functions into the standard inference procedure of pre-trained diffusion models. Each step is designed to approximately sample from a value-weighted policy {p⋆,α t }0 t=T . We have several important things to note. • When α = 0, Line 4 corresponds to ζt−1 = argmaxm∈[1,··· ,M] ˆ vt−1(x⟨m⟩ t−1). In practice, we often recommend this choice. • A typical choice we recommend is M = [5, · · · , 20]. The performance with varying M values will be discussed in Section 6. • Line 3 can be computed in parallel at the expense of additional memory. • When the normalizing constant can be calculated relatively easily (e.g., in discrete diffusion with small K, L), we can directly sample from {p⋆,α t }0 t=T . The remaining question is how to obtain the soft value function, which we address in the next section. 4.3 Learning Soft Value Functions Next, we describe how to learn soft value functions vt(x) in practice. We propose two main approaches: the Monte Carlo regression approach and the posterior mean approximation approach. Monte Carlo regression. Here, we use the following approximation v′ t as vt where v′ t(·) := Ex0∼ppre(x0|xt)[r(x0)|xt = ·]. 6 \n",
      "\n",
      "Page 7 Text:\n",
      "This is based on vt(xt) := α log Ex0∼ppre(x0|xt)[exp(r(x0)/α)|xt] ≈log(exp(Ex0∼ppre(x0|xt)[r(x0)|xt])) = v′ t(xt). (1) By regressing r(x0) onto xt, we can learn v′ t as in Algorithm 2. Combining this with Algorithm 1, we refer to the entire optimization approach as SVDD-MC. Algorithm 2 Value Function Estimation Using Monte Carlo Regression 1: Require: Pre-trained diffusion models, reward r : X →R, function class Φ : X × [0, T] →R. 2: Collect datasets {x(s) T , · · · , x(s) 0 }S s=1 by rolling-out {ppre t }0 t=T from t = T to t = 0. 3: ˆ v′ = argminf∈Φ PT t=0 PS s=1{r(x(s) 0 ) −f(x(s) t , t)}2. 4: Output: ˆ v′ Note that technically, without approximation introduced in (1), we can estimate vt by regressing exp(r(x0)/α) onto xt based on the original definition. This approach may work in many cases. However, when α is very small, the scaling of exp(r(·)/α) tends to be excessively large. Due to this concern, we generally recommend using using Algorithm 2. Remark 1 (Another way of constructing datasets). We have used the backward process in pre-trained diffusion models to collect training datasets in Algorithm 2. Given that the backward and forward processes are designed to match marginally during the training of diffusion models as discussed in Section 3.1, we can also use the forward process in pre-trained diffusion models to construct datasets. Remark 2 (Another way of learning value functions). Technically, another method for learning value functions is available such as soft-Q-learning (Section B), by leveraging soft-Bellman equations in diffusion models Uehara et al. (2024, Section 3) (as known in standard RL contexts (Levine, 2018; Haarnoja et al., 2017; Geist et al., 2019)). However, since we find Monte Carlo approaches to be more stable, we recommend them over soft-Q-learning. Posterior mean approximation. Here, recalling we use ˆ x0(xt) (approximation of Ex0∼ppre(xt)[x0|xt]) when training pre-trained diffusion models in Section 3.1, we perform the following approximation: vt(x) := α log Ex0∼ppre(x0|xt)[exp(r(x0)/α)|xt] ≈α log(exp(r(ˆ x0(xt))/α) = r(ˆ x0(xt)). Then, we can use r(ˆ x0(xt)) as the estimated value function. The advantage of this approach is that no additional training is required as long as we have r. When combined with Algorithm 1, we refer to the entire approach as SVDD-PM. Algorithm 3 Value Function Estimation using Posterior Mean Approximation 1: Require: Pre-trained diffusion models, reward r : X →R 2: Set ˆ v⋄(·, t) := r(ˆ x0(xt = ·), t) 3: Output: ˆ v⋄ Remark 3 (Relation with DPS). In the context of classifier guidance, similar approximations have been employed (e.g., DPS in Chung et al. (2022)). However, the final inference-time algorithms differ significantly, as these methods compute gradients at the end. 5 Advantages, Limitations, and Extensions of SVDD So far, we have detailed our algorithm, SVDD. In this section, we discuss its advantages, limitations, and extensions. 5.1 Advantages No fine-tuning (or no training in SVDD-PM). Unlike classifier-free guidance or RL-based fine- tuning, SVDD does not require any fine-tuning of the generative models. In particular, when using SVDD-PM, no additional training is needed as long as we have r. 7 \n",
      "\n",
      "Page 8 Text:\n",
      "No need for constructing differentiable models. Unlike classifier guidance, SVDD does not require differentiable proxy models, as there is no need for derivative computations. For example, if r is non-differentiable feedback (e.g., physically-based simulations, QED, SA in molecule generation), our method SVDD-PM can directly utilize such feedback without constructing differentiable proxy models. In cases where non-differentiable feedback is costly to obtain, proxy reward models may still be required, but they do not need to be differentiable; thus, non-differentiable features or non- differentiable models based on scientific knowledge (e.g., molecule fingerprints, GNNs) can be leveraged. Similarly, when using SVDD-MC, while a value function model is required, it does not need to be differentiable, unlike classifier guidance. Additionally, compared to approaches that involve derivatives (like classifier guidance or DPS), our algorithm may be more memory efficient at inference time, particularly when M is moderate, and can be directly applied to discrete diffusion models mentioned in Example 2. Proximity to pre-trained models (robust to reward over-optimization). Since samples are consistently generated from pre-trained diffusion policies at each step, they are ensured to remain within the natural space defined by pre-trained diffusion models. This is especially advantageous when rewards to be optimized by SVDD are learned from offline data. In such cases, learned reward functions may be inaccurate for out-of-distribution samples (Uehara et al., 2024). Consequently, conventional fine-tuning methods often suffer from over-optimization (reward hacking) by exploiting these out-of-distribution regions (Fan et al., 2023; Clark et al., 2023). Given that non-natural design spaces typically encompass a significant portion of out-of-distribution regions of the offline data, maintaining proximity to pre-trained models acts as a regularization mechanism against these regions. 5.2 Potential Limitations Memory and computational complexity in inference time. Our approach requires more compu- tational resources (if not parallelized) or memory (if parallelized), specifically M times more than standard inference methods. Proximity to pre-trained models. The proximity to pre-trained models might be a disadvantage if significant changes to the pre-trained models are desired. We acknowledge that RL-based fine-tuning could be more effective for this purpose than our algorithm. 5.3 Extensions Using a likelihood/classifier as a reward. While we primarily consider scenarios where reward models are regression models, by adopting a similar strategy in (Zhao et al., 2024), they can be readily replaced with classifiers or likelihood functions in the context of solving inverse problems (Chung et al., 2022; Bansal et al., 2023). Combination with sequential Monte Carlo. SVDD iterates IS and resampling at each team step locally, but our algorithm can be combined with the more sophisticated global resampling method known as particle filter method (i.e., sequential Monte Carlo) (Del Moral and Doucet, 2014; Kitagawa, 1993; Doucet et al., 2009). Although we do not recommend this approach in practice due to its difficulty in parallelization, we discuss its extension in Section A. Application to fine-tuning. Our SVDD, can also be naturally extended to fine-tuning by generating samples with SVDD, and then using these samples for supervised fine-tuning. 6 Experiments We conduct experiments to assess the performance of our algorithm relative to baselines and its sensitivity to various hyperparameters. We start by outlining the experimental setup, including baselines and models, and then present the results. 8 \n",
      "\n",
      "Page 9 Text:\n",
      "6.1 Settings Methods to compare. We compare the following methods. • Pre-trained models: We generate samples using pre-trained models. • Best-of-N (Nakano et al., 2021): We generate samples from pre-trained models and select the top 1/N samples. This selection is made to ensure that the computational time during inference is approximately equivalent to that of our proposal. • DPS (Chung et al., 2022): DPS is a widely used enhancement of classifier guidance. Although the original work was not designed for discrete diffusion, we employ specific heuristics to adapt it for this purpose (Section C). • SVDD (Ours): We implement SVDD-MC and SVDD-PM. We generally set M = 20 for images and M = 10 for other domains, additionally we set α = 0. Datasets and reward models. We provide details on the pre-trained diffusion models and downstream reward functions used. For further information, refer to Section C. • Images: We use Stable Diffusion v1.5 as the pre-trained diffusion model (T = 50). For downstream reward functions, we use compressibility and aesthetic scores (LAION Aesthetic Predictor V2 in Schuhmann (2022)), as employed in Black et al. (2023); Fan et al. (2023). Note that compressibility is non-differentiable reward feedback. • Molecules: We use GDSS (Jo et al., 2022), trained on ZINC-250k (Irwin and Shoichet, 2005), as the pre-trained diffusion model (T = 1000). For downstream reward functions, we use QED and SA calculated by RDKit, which are non-differentiable feedback. Here, we renormalize SA to (10 −SA)/9 so that a higher value indicates better performance. • DNAs (Enhancers): We use the discrete diffusion model (Sahoo et al., 2024), trained on datasets from Gosai et al. (2023), as our pre-trained diffusion model (T = 128). For the downstream reward function, we use an Enformer model (Avsec et al., 2021) to predict activity in the HepG2 cell line. • RNAs (5’UTRs): We use the discrete diffusion model (Sahoo et al., 2024) as the pre-trained diffusion model (T = 128). For downstream reward functions, we employ a reward model that predicts the mean ribosomal load (MRL) measured by polysome profiling (trained on datasets from Sample et al. (2019)) and stability (trained on datasets from (Agarwal and Kelley, 2022)). 6.2 Results (a) Images: compress- ibility (b) Images: aesthetic score (c) Molecules: QED (d) Molecules: SA (e) Enhancers (f) 5’UTRs: MRL (g) 5’UTRs: stability Figure 2: We show the histogram of generated samples in terms of reward functions. We compare the baselines with our two proposals. The performance is shown in Figure 2, and the generated samples are presented in Figure 3. 9 \n",
      "\n",
      "Page 10 Text:\n",
      "(a) Images: compressibility (b) Images: aesthetic scores (c) Molecules: QED scores (d) Molecules: SA scores (Normalized as (10 − SA)/9 ) Figure 3: We show generated samples from our proposal. For more samples, refer to Section C.2. Overall, our proposal outperforms the baseline methods (Best-of-N and DPS), as evidenced by higher rewards for samples generated in large quantities. More formally, in Section C, we compare the top 10 and 20 quantiles from each algorithm and confirm that SVDD always outperforms the baselines. This indicates that our algorithm can generate high-reward samples that Best-of-N and DPS cannot. • Compared to Best-of-N, although the rewards for generating samples in smaller quantities could be lower with our algorithm, this is expected because our algorithm generates samples with high likelihood ppre(x|c) in pre-trained diffusion models, but with possibly lower rewards. • Compared to DPS, our algorithm consistently outperforms. Notably, in molecular generation, DPS is highly ineffective due to the non-differentiable nature of the original feedback. The superiority of our two proposals (SVDD-MC or SVDD-PM) appears to be domain-dependent. Generally, SVDD-PM may be more robust since it does not require additional learning (i.e., it directly utilizes reward feedback). The performance of SVDD-MC depends on the success of value function learning, which is discussed in Section C. Figure 4: Performance of our algorithm SVDD as M varies, for image generation while op- timizing the aesthetic score. Ablation studies in terms of M. We plot the performance as M varies. The performance gradually reaches a plateau as M increases. This tendency is seen in all other domains. 7 Conclusion We propose a novel inference-time algorithm, SVDD, for optimizing downstream reward functions in pre-trained diffusion models that eliminate the need to construct differentiable proxy models. In future work, we plan to conduct experiments in other domains, such as protein sequence optimization (Gruver et al., 2023; Alamdari et al., 2023; Watson et al., 2023) or controllable 3D molecule generation (Xu et al., 2023). 10 \n",
      "\n",
      "Page 11 Text:\n",
      "References Agarwal, V. and D. R. Kelley (2022). The genetic and biochemical determinants of mrna degradation rates in mammals. Genome biology 23(1), 245. Alamdari, S., N. Thakkar, R. van den Berg, A. X. Lu, N. Fusi, A. P. Amini, and K. K. Yang (2023). Protein generation with evolutionary diffusion: sequence is all you need. bioRxiv, 2023–09. Alford, R. F., A. Leaver-Fay, J. R. Jeliazkov, M. J. O’Meara, F. P. DiMaio, H. Park, M. V. Shapovalov, P. D. Renfrew, V. K. Mulligan, K. Kappel, et al. (2017). The rosetta all-atom energy function for macromolecular modeling and design. Journal of chemical theory and computation 13(6), 3031–3048. Alhossary, A., S. D. Handoko, Y. Mu, and C.-K. Kwoh (2015). Fast, accurate, and reliable molecular docking with quickvina 2. Bioinformatics 31(13), 2214–2216. Asrani, K. H., J. D. Farelli, M. R. Stahley, R. L. Miller, C. J. Cheng, R. R. Subramanian, and J. M. Brown (2018). Optimization of mrna untranslated regions for improved expression of therapeutic mrna. RNA biology 15(6), 756–762. Austin, J., D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg (2021). Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems 34, 17981–17993. Avdeyev, P., C. Shi, Y. Tan, K. Dudnyk, and J. Zhou (2023). Dirichlet diffusion score model for biological sequence generation. arXiv preprint arXiv:2305.10699. Avsec, ˇ Z., V. Agarwal, D. Visentin, J. R. Ledsam, A. Grabska-Barwinska, K. R. Taylor, Y. Assael, J. Jumper, P. Kohli, and D. R. Kelley (2021). Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods 18(10), 1196–1203. Bansal, A., H.-M. Chu, A. Schwarzschild, S. Sengupta, M. Goldblum, J. Geiping, and T. Goldstein (2023). Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 843–852. Beirami, A., A. Agarwal, J. Berant, A. D’Amour, J. Eisenstein, C. Nagpal, and A. T. Suresh (2024). Theoretical guarantees on the best-of-n alignment policy. arXiv preprint arXiv:2401.01879. Black, K., M. Janner, Y. Du, I. Kostrikov, and S. Levine (2023). Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301. Campbell, A., J. Benton, V. De Bortoli, T. Rainforth, G. Deligiannidis, and A. Doucet (2022). A continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems 35, 28266–28279. Campbell, A., J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola (2024). Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997. Castillo-Hair, S. M. and G. Seelig (2021). Machine learning for designing next-generation mrna therapeutics. Accounts of Chemical Research 55(1), 24–34. Cheng, F., Y. Wang, Y. Bai, Z. Liang, Q. Mao, D. Liu, X. Wu, and M. Xu (2023). Research advances on the stability of mrna vaccines. Viruses 15(3), 668. Chorowski, J. and N. Jaitly (2016). Towards better decoding and language model integration in sequence to sequence models. arXiv preprint arXiv:1612.02695. Chung, H., J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye (2022). Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687. Clark, K., P. Vicol, K. Swersky, and D. J. Fleet (2023). Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400. 11 \n",
      "\n",
      "Page 12 Text:\n",
      "Dathathri, S., A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu (2019). Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv:1912.02164. Del Moral, P. and A. Doucet (2014). Particle methods: An introduction with applications. In ESAIM: proceedings, Volume 44, pp. 1–46. EDP Sciences. Dey, R. and F. M. Salem (2017). Gate-variants of gated recurrent unit (gru) neural networks. In 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS), pp. 1597–1600. IEEE. Dhariwal, P. and A. Nichol (2021). Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34, 8780–8794. Dong, H., W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang (2023). Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767. Doucet, A., A. M. Johansen, et al. (2009). A tutorial on particle filtering and smoothing: Fifteen years later. Handbook of nonlinear filtering 12(656-704), 3. Emonts, J. and J. F. Buyel (2023). An overview of descriptors to capture protein properties–tools and perspectives in the context of qsar modeling. Computational and Structural Biotechnology Journal 21, 3234–3247. Fan, Y., O. Watkins, Y. Du, H. Liu, M. Ryu, C. Boutilier, P. Abbeel, M. Ghavamzadeh, K. Lee, and K. Lee (2023). DPOK: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381. Gao, L., J. Schulman, and J. Hilton (2023). Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 10835–10866. PMLR. Geist, M., B. Scherrer, and O. Pietquin (2019). A theory of regularized markov decision processes. In International Conference on Machine Learning, pp. 2160–2169. PMLR. Ghiringhelli, L. M., J. Vybiral, S. V. Levchenko, C. Draxl, and M. Scheffler (2015). Big data of materials science: critical role of the descriptor. Physical review letters 114(10), 105503. Gosai, S. J., R. I. Castro, N. Fuentes, J. C. Butts, S. Kales, R. R. Noche, K. Mouri, P. C. Sabeti, S. K. Reilly, and R. Tewhey (2023). Machine-guided design of synthetic cell type-specific cis-regulatory elements. bioRxiv. Gruver, N., S. Stanton, N. C. Frey, T. G. Rudner, I. Hotzel, J. Lafrance-Vanasse, A. Rajpal, K. Cho, and A. G. Wilson (2023). Protein design with guided discrete diffusion. arXiv preprint arXiv:2305.20009. Guo, Y., H. Yuan, Y. Yang, M. Chen, and M. Wang (2024). Gradient guidance for diffusion models: An optimization perspective. arXiv preprint arXiv:2404.14743. Haarnoja, T., H. Tang, P. Abbeel, and S. Levine (2017). Reinforcement learning with deep energy- based policies. In International conference on machine learning, pp. 1352–1361. PMLR. Han, S., I. Shenfeld, A. Srivastava, Y. Kim, and P. Agrawal (2024). Value augmented sampling for language model alignment and personalization. arXiv preprint arXiv:2405.06639. Hayes, T., R. Rao, H. Akin, N. J. Sofroniew, D. Oktay, Z. Lin, R. Verkuil, V. Q. Tran, J. Deaton, M. Wiggert, et al. (2024). Simulating 500 million years of evolution with a language model. bioRxiv, 2024–07. Ho, J., A. Jain, and P. Abbeel (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems 33, 6840–6851. Ho, J. and T. Salimans (2022). Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598. Ho, J., T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet (2022). Video diffusion models. Advances in Neural Information Processing Systems 35, 8633–8646. 12 \n",
      "\n",
      "Page 13 Text:\n",
      "Irwin, J. J. and B. K. Shoichet (2005). ZINC- a free database of commercially available compounds for virtual screening. Journal of chemical information and modeling 45(1), 177–182. Jin, W., R. Barzilay, and T. Jaakkola (2018). Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pp. 2323–2332. PMLR. Jo, J., S. Lee, and S. J. Hwang (2022). Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning, pp. 10362– 10383. PMLR. Kitagawa, G. (1993). A monte carlo filtering and smoothing method for non-gaussian nonlinear state space models. In Proceedings of the 2nd US-Japan joint seminar on statistical time series analysis, Volume 110. Lal, A., D. Garfield, T. Biancalani, and G. Eraslan (2024). reglm: Designing realistic regulatory dna with autoregressive language models. bioRxiv, 2024–02. Leblond, R., J.-B. Alayrac, L. Sifre, M. Pislar, J.-B. Lespiau, I. Antonoglou, K. Simonyan, and O. Vinyals (2021). Machine translation decoding beyond beam search. arXiv preprint arXiv:2104.05336. Lee, S., J. Jo, and S. J. Hwang (2023). Exploring chemical space with score-based out-of-distribution generation. In International Conference on Machine Learning, pp. 18872–18892. PMLR. Levine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909. Lew, A. K., T. Zhi-Xuan, G. Grand, and V. K. Mansinghka (2023). Sequential monte carlo steering of large language models using probabilistic programs. arXiv preprint arXiv:2306.03081. Lou, A., C. Meng, and S. Ermon (2023). Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834. Mudgal, S., J. Lee, H. Ganapathy, Y. Li, T. Wang, Y. Huang, Z. Chen, H.-T. Cheng, M. Collins, T. Strohman, et al. (2023). Controlled decoding from language models. arXiv preprint arXiv:2310.17022. Nakano, R., J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. (2021). Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. Nisonoff, H., J. Xiong, S. Allenspach, and J. Listgarten (2024). Unlocking guidance for discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572. Prabhudesai, M., A. Goyal, D. Pathak, and K. Fragkiadaki (2023). Aligning text-to-image diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739. Qin, L., S. Welleck, D. Khashabi, and Y. Choi (2022). Cold decoding: Energy-based constrained text generation with langevin dynamics. Advances in Neural Information Processing Systems 35, 9538–9551. Sahoo, S. S., M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin, J. T. Chiu, A. Rush, and V. Kuleshov (2024). Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524. Sample, P. J., B. Wang, D. W. Reid, V. Presnyak, I. J. McFadyen, D. R. Morris, and G. Seelig (2019). Human 5’utr design and variant effect prediction from a massively parallel translation assay. Nature biotechnology 37(7), 803–809. Sarkar, A., Z. Tang, C. Zhao, and P. Koo (2024). Designing dna with tunable regulatory activity using discrete diffusion. bioRxiv, 2024–05. Schuhmann, C. (2022, Aug). LAION aesthetics. Shi, J., K. Han, Z. Wang, A. Doucet, and M. K. Titsias (2024). Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329. 13 \n",
      "\n",
      "Page 14 Text:\n",
      "Shi, Y., V. De Bortoli, A. Campbell, and A. Doucet (2024). Diffusion schr¨ odinger bridge matching. Advances in Neural Information Processing Systems 36. Sohl-Dickstein, J., E. Weiss, N. Maheswaranathan, and S. Ganguli (2015). Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 2256–2265. PMLR. Song, J., C. Meng, and S. Ermon (2020). Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Song, Y., J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole (2020). Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456. Stark, H., B. Jing, C. Wang, G. Corso, B. Berger, R. Barzilay, and T. Jaakkola (2024). Dirichlet flow matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841. Stiennon, N., L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano (2020). Learning to summarize with human feedback. Advances in Neural Information Processing Systems 33, 3008–3021. Taskiran, I. I., K. I. Spanier, H. Dickm¨ anken, N. Kempynck, A. Panˇ c´ ıkov´ a, E. C. Eks ¸i, G. Hulselmans, J. N. Ismail, K. Theunis, R. Vandepoel, et al. (2024). Cell-type-directed design of synthetic enhancers. Nature 626(7997), 212–220. Touvron, H., L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Trott, O. and A. J. Olson (2010). Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry 31(2), 455–461. Uehara, M., Y. Zhao, T. Biancalani, and S. Levine (2024). Understanding reinforcement learning- based fine-tuning of diffusion models: A tutorial and review. arXiv preprint arXiv:2407.13734. Uehara, M., Y. Zhao, K. Black, E. Hajiramezanali, G. Scalia, N. L. Diamant, A. M. Tseng, T. Bian- calani, and S. Levine (2024). Fine-tuning of continuous-time diffusion models as entropy- regularized control. arXiv preprint arXiv:2402.15194. Uehara, M., Y. Zhao, E. Hajiramezanali, G. Scalia, G. Eraslan, A. Lal, S. Levine, and T. Biancalani (2024). Bridging model-based optimization and generative modeling via conservative fine-tuning of diffusion models. arXiv preprint arXiv:2405.19673. van Westen, G. J., R. F. Swier, I. Cortes-Ciriano, J. K. Wegner, J. P. Overington, A. P. IJzerman, H. W. van Vlijmen, and A. Bender (2013). Benchmarking of protein descriptor sets in proteochemo- metric modeling (part 2): modeling performance of 13 amino acid descriptor sets. Journal of cheminformatics 5, 1–20. Vignac, C., I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard (2022). Digress: Discrete denoising diffusion for graph generation. arXiv preprint arXiv:2209.14734. Wang, Y., J. Yu, and J. Zhang (2022). Zero-shot image restoration using denoising diffusion null-space model. arXiv preprint arXiv:2212.00490. Watson, J. L., D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, et al. (2023). De novo design of protein structure and function with rfdiffusion. Nature 620(7976), 1089–1100. Wu, Y., M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al. (2016). Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144. Xu, K., W. Hu, J. Leskovec, and S. Jegelka (2018). How powerful are graph neural networks? arXiv preprint arXiv:1810.00826. 14 \n",
      "\n",
      "Page 15 Text:\n",
      "Xu, M., A. S. Powers, R. O. Dror, S. Ermon, and J. Leskovec (2023). Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pp. 38592– 38610. PMLR. Yang, K. and D. Klein (2021). Fudge: Controlled text generation with future discriminators. arXiv preprint arXiv:2104.05218. Yu, J., Y. Wang, C. Zhao, B. Ghanem, and J. Zhang (2023). Freedom: Training-free energy-guided conditional diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23174–23184. Zhao, S., R. Brekelmans, A. Makhzani, and R. Grosse (2024). Probabilistic inference in language models via twisted sequential monte carlo. arXiv preprint arXiv:2404.17546. Zhao, Y., M. Uehara, G. Scalia, T. Biancalani, S. Levine, and E. Hajiramezanali (2024). Adding con- ditional control to diffusion models with reinforcement learning. arXiv preprint arXiv:2406.12120. 15 \n",
      "\n",
      "Page 16 Text:\n",
      "Algorithm 4 Decoding via Sequential Monte Carlo in Diffusion Models 1: Require: Estimated value functions {ˆ vt(x)}0 t=T , pre-trained diffusion models {ppre t }0 t=T , hyper- parameter α ∈R, Batch size N 2: for t ∈[T + 1, · · · , 0] do 3: IS step: 4: i ∈[1, · · · , N]; x[i] t−1 ∼ppre t−1(·|x[i] t ), w[i] t−1 := exp(ˆ vt−1(x[i] t−1)/α) exp(ˆ vt(x[i] t )/α) 5: Selection step: select new indices with replacement 6: {x[i] t−1}N i=1 ←{x ζ[i] t−1 t−1 }N i=1, {ζ[i] t−1}N i=1 ∼Cat \u001a w[i] t−1 PN j=1 w[j] t−1 \u001bN i=1 ! 7: end for 8: Output: x0 A Soft Value-Based Decoding with Particle Filter In this section, we explain soft-value decoding incorporating particle filtering (Doucet et al., 2009). However, we generally do not recommend this approach in practice due to its difficulty in paralleliza- tion. The complete algorithm is summarized in Algorithm 4. Here, we provide a brief overview. It consists of two steps. Since our algorithm is iterative, at time point t, consider we have N samples (particles) {x[i] t }i = 1N. IS step (line 3). We generate a set of samples {x[i] t−1}N i=1 following a policy from a pre-trained model ppre t−1(·|·). In other words, ∀i ∈[1, · · · , N]; x[i] t−1 ∼ppre t−1(·|x[i] t ). Now, we denote the importance weight for the next particle xt−1 given the current particle xt as w(xt−1, xt), expressed as w(xt−1, xt) := exp(vt−1(xt−1)/α) R exp(vt−1(xt−1)/α)ppre t−1(xt−1|xt)dxt−1 = exp(vt−1(xt−1)/α) exp(vt(xt)/α) , and define ∀i ∈[1, · · · , N]; w[i] t−1 := w(x[i] t−1, x[i] t ). Note here we have used the soft Bellman equation: exp(vt(xt)/α) = Z exp(vt−1(xt−1)/α)ppre t−1(xt−1|xt)dxt−1. Hence, by denoting the target marginal distribution at t −1, we have the following approximation: ptar t−1 ≈ |{z} IS N X i=1 w[i] t−1 PN j=1 w[j] t−1 δx[i] t−1. Selection step (line 5). Finally, we consider a resampling step. The resampling indices are determined by the following: {ζ[i] t−1}N i=1 ∼Cat   ( w[i] t−1 PN j=1 w[j] t−1 )N i=1  . To summarize, we conduct ptar t−1 ≈ |{z} IS N X i=1 w[i] t−1 PN j=1 w[j] t−1 δx[i] t−1 ≈ |{z} Resampling 1 N N X i=1 δ x ζ[i] t−1 t−1 . Remark 4 (Works in autoregressive models). We note that in the context of autoregressive (language) models, Zhao et al. (2024); Lew et al. (2023) proposed a similar algorithm. 16 \n",
      "\n",
      "Page 17 Text:\n",
      "Algorithm 5 Value Function Estimation Using Soft Q-learning 1: Require: Pre-trained diffusion models {ppre t }0 t=T , value function model v(x; θ) 2: Collect datasets {x(s) T , · · · , x(s) 0 }S s=1 by rolling-out {ppre t }0 t=T from t = T to t = 0. 3: for j ∈[0, · · · , J] do 4: Update θ by running regression: θ′ j ←argmin θ T X t=0 S X s=1 n v(x(s) t ; θ) −v(x(s) t−1; θ′ j−1) o2 . 5: end for 6: Output: v(x; θ′ J) B Soft Q-learning In this section, we explain soft value iteration to estimate soft value functions, which serves as an alternative to Monte Carlo regression. Soft Bellman equation. Here, we use the soft Bellman equation: exp(vt(xt)/α) = Z exp(vt−1(xt−1)/α)ppre t−1(xt−1|xt)dxt−1, as proved in Section 4.1 in (Uehara et al., 2024). In other words, vt(xt) = α log{Ext−1∼ppre(·|xt) [exp(vt−1(xt−1)/α)|xt]}. Algorithm. Based on the above, we can estimate soft value functions recursively by regressing vt−1(xt−1) onto xt. This approach is often referred to as soft Q-learning in the reinforcement learning literature (Haarnoja et al., 2017; Levine, 2018). In our context, due to the concern of scaling of α, as we have done in Algorithm 2, we had better use vt(xt) = Ext−1∼ppre(·|xt) [vt−1(xt−1)|xt] . With the above recursive equation, we can estimate soft value functions as in Algorithm 5. C Additional Experimental Details We further add additional experimental details. C.1 Additional Setups for Experiments Images. • DPS: We require differentiable models that map images to compressibility. For this task, we have used a standard CNN. Molecules. • DPS: Following the implementation in Lee et al. (2023), we use the same GNN model as the reward model. Note that this model cannot compute derivatives with respect to adjacency matrices. • SVDD-MC: We use a Graph Isomorphism Network (GIN) model (Xu et al., 2018) as a value function model. Enhancers. • DPS: Although DPS was originally proposed in the continuous space, we have adapted it for our use by incorporating the gradient of the value function model at each step and representing each sequence as a one-hot encoding vector. • SVDD-MC: We have used the Enformer model as the function model. 17 \n",
      "\n",
      "Page 18 Text:\n",
      "5’UTRs. We have used ConvGRU as the reward model (Dey and Salem, 2017). • DPS: Although DPS was originally proposed in the continuous space, we have adapted it for our use by incorporating the gradient of the value function model at each step and representing each sequence as a one-hot encoding vector. • SVDD-MC: We employed ConvGRU as the value function model. C.2 Additional Results We evaluate the performance of the generated samples using histograms in Section 6. Here, we present the top 10 and 20 quantiles of the generated samples in Table 2. Table 2: Top 10 and 20 quantiles of the generated samples for each algorithm. Higher is better. Domain Quantile Pre-Train Best-N DPS SVDD-MC SVDD-PM Images compress 20% -86.2 -63.2 -67.1 - -43.7 10% -78.6 -57.3 -61.2 - -38.8 Images aesthetic 20% 5.875 6.246 5.868 - 6.356 10% 5.984 6.343 5.997 - 6.472 Molecules QED 20% 0.771 0.881 0.802 0.912 0.916 10% 0.812 0.902 0.843 0.925 0.928 Molecules SA 20% 0.750 0.916 0.802 1.0 1.0 10% 0.803 0.941 0.849 1.0 1.0 Enhancers 20% 0.74 3.00 2.68 5.53 6.44 10% 1.41 3.52 3.85 5.75 7.02 5’UTR MRL 20% 0.78 0.97 0.90 1.09 1.33 10% 0.86 1.021 0.93 1.12 1.38 5’UTR Stability 20% -0.63 -0.59 -0.62 -0.52 -0.56 10% -0.61 -0.58 -0.60 -0.51 -0.55 Performance of value function training. We report the performance of value function learning using Monte Carlo regression as follows. We plot the Pearson correlation on the test dataset. (a) Images: compressibil- ity (b) Images: aesthetic score (c) Molecules: QED Figure 5: Training curve More generated samples. We have provided additional generated samples in Figure 6, Figure 7, Figure 8 and Figure 9. 18 \n",
      "\n",
      "Page 19 Text:\n",
      "-39 -20 -39 -31 -43 -37 -74 -61 -73 -68 -81 -79 Best-N SVDD-PM -112 -130 Pre-trained -103 -155 -97 -103 Figure 6: Additional generated samples (Domain: images, Reward: Compressibility) 6.5 6.5 6.6 6.9 6.5 6.5 6.0 5.8 5.9 5.7 6.1 6.1 Best-N SVDD-PM 5.5 5.6 Pre-trained 5.7 5.5 5.6 5.3 Figure 7: Additional generated samples (Domain: Images, Reward: Aesthetic score) 19 \n",
      "\n",
      "Page 20 Text:\n",
      "Figure 8: Additional generated samples (Domain: Molecules, Reward: QED score) Figure 9: Additional generated samples (Domain: Molecules, Reward: SA score, normalized as (10 −SA)/9) 20 \n",
      "\n",
      "Page 21 Text:\n",
      "Figure 10: Additional generated samples from SVDD(Domain: Molecules, Reward: SA score = 1.0 (normalized as (10 −SA)/9)) 21 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0cf8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "host = \"127.0.0.1\"\n",
    "port = \"11434\"\n",
    "client = ollama.Client(host=f\"http://{host}:{port}\")\n",
    "\n",
    "# Create the prompt to categorize and summarize the sections\n",
    "prompt = f\"\"\"\n",
    "You are a researcher given a string containing page-wise data from a research paper:\n",
    "{full_text}\n",
    "\n",
    "#########\n",
    "Summarize each section of the paper based on the provided data. The sections to be summarized are: Introduction, Methodology, and Results and Discussion. Follow these instructions for each section:\n",
    "\n",
    "For each section found, summarize it as follows:\n",
    "1. **Introduction:** Summarize the background information, research problem, objectives, and significance of the study. Highlight the research questions or hypotheses being addressed.\n",
    "2. **Methodology:** Outline the research design, methods, and procedures used in the study. Include information on data collection, analysis techniques, and any specific tools or materials used.\n",
    "3. **Results and Discussion:** Summarize the key findings of the research, including data and observations, and provide a summary of the interpretation of the results. Discuss how the findings relate to the research questions, hypotheses, and existing literature. Note any implications or limitations.\n",
    "\n",
    "Do not use too many technical terms. If you are using technical terms, explain what they mean so that even non-technical people can understand the summaries.\n",
    "\n",
    "Write the summaries using neutral language. Describe the research, methods, and findings directly, focusing solely on the content. Avoid references to sections, phrases like \"the paper states\" or \"the section explains,\" and any language that implies a viewpoint or assumption. Use phrases that attribute actions and results to the research itself, such as \"The research used a combination,\" instead of \"The researchers used a combination.\"\n",
    "\"\"\"\n",
    "\n",
    "# Send the request to the model\n",
    "res = client.chat(\n",
    "    model=\"llama3.1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    options={\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a784079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import ollama\\n\\nhost = \"127.0.0.1\"\\nport = \"11434\"\\nclient = ollama.Client(host=f\"http://{host}:{port}\")\\n\\n# Create the prompt to categorize and summarize the sections\\nprompt = f\"\"\"\\nYou are an expert in academic research tasked with summarizing a research paper. The input string contains page-wise data from the paper. Your goal is to provide clear, concise, and non-technical summaries for the following sections: Introduction, Methodology, and Results and Discussion.\\n\\nPlease ensure that you:\\n\\n1. **Focus on Specific Sections**: Only summarize content that corresponds to the Introduction, Methodology, and Results and Discussion. Ignore any other sections, such as appendices, abstracts, or unrelated content.\\n\\n2. **Infer and Summarize**: Even if the exact titles \"Introduction,\" \"Methodology,\" or \"Results and Discussion\" are not explicitly mentioned, generate summaries based on the content that most closely matches these sections:\\n    - **Introduction:** Summarize the background information, research problem, objectives, and significance of the study. Clearly identify the research questions or hypotheses being addressed.\\n    - **Methodology:** Provide a detailed outline of the research design, methods, and procedures. Include information on data collection, analysis techniques, and any tools or materials used.\\n    - **Results and Discussion:** Summarize the key findings, data, and observations. Provide an interpretation of the results and discuss their relevance to the research questions or hypotheses. Mention how the findings compare to existing literature and any implications or limitations.\\n\\n3. **Simplify**: Use straightforward language, explaining any technical terms to ensure the summaries are understandable to non-experts.\\n\\n4. **Neutral Language**: Write the summaries using neutral language. Avoid introducing opinions, assumptions, or unnecessary details. Focus solely on the content relevant to each section.\\n\\n5. **Format**: Format your response exactly as follows, ensuring each section is clearly labeled:\\n    ```\\n    **Introduction:**\\n    [Your summarized text here]\\n\\n    **Methodology:**\\n    [Your summarized text here]\\n\\n    **Results and Discussion:**\\n    [Your summarized text here]\\n    ```\\n\\n6. **Generate Content**: If the content seems to be missing, make a logical inference based on the surrounding text to construct a plausible summary for each of these sections.\\n\\n**Input Data:**\\n{full_text}\\n\"\"\"\\n\\n# Send the request to the model\\nres = client.chat(\\n    model=\"llama3.1\",\\n    messages=[{\"role\": \"user\", \"content\": prompt}],\\n    options={\\n        \"temperature\": 0.5,\\n        \"top_p\": 0.9,\\n    }\\n)\\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import ollama\n",
    "\n",
    "host = \"127.0.0.1\"\n",
    "port = \"11434\"\n",
    "client = ollama.Client(host=f\"http://{host}:{port}\")\n",
    "\n",
    "# Create the prompt to categorize and summarize the sections\n",
    "prompt = f\"\"\"\n",
    "You are an expert in academic research tasked with summarizing a research paper. The input string contains page-wise data from the paper. Your goal is to provide clear, concise, and non-technical summaries for the following sections: Introduction, Methodology, and Results and Discussion.\n",
    "\n",
    "Please ensure that you:\n",
    "\n",
    "1. **Focus on Specific Sections**: Only summarize content that corresponds to the Introduction, Methodology, and Results and Discussion. Ignore any other sections, such as appendices, abstracts, or unrelated content.\n",
    "\n",
    "2. **Infer and Summarize**: Even if the exact titles \"Introduction,\" \"Methodology,\" or \"Results and Discussion\" are not explicitly mentioned, generate summaries based on the content that most closely matches these sections:\n",
    "    - **Introduction:** Summarize the background information, research problem, objectives, and significance of the study. Clearly identify the research questions or hypotheses being addressed.\n",
    "    - **Methodology:** Provide a detailed outline of the research design, methods, and procedures. Include information on data collection, analysis techniques, and any tools or materials used.\n",
    "    - **Results and Discussion:** Summarize the key findings, data, and observations. Provide an interpretation of the results and discuss their relevance to the research questions or hypotheses. Mention how the findings compare to existing literature and any implications or limitations.\n",
    "\n",
    "3. **Simplify**: Use straightforward language, explaining any technical terms to ensure the summaries are understandable to non-experts.\n",
    "\n",
    "4. **Neutral Language**: Write the summaries using neutral language. Avoid introducing opinions, assumptions, or unnecessary details. Focus solely on the content relevant to each section.\n",
    "\n",
    "5. **Format**: Format your response exactly as follows, ensuring each section is clearly labeled:\n",
    "    ```\n",
    "    **Introduction:**\n",
    "    [Your summarized text here]\n",
    "\n",
    "    **Methodology:**\n",
    "    [Your summarized text here]\n",
    "\n",
    "    **Results and Discussion:**\n",
    "    [Your summarized text here]\n",
    "    ```\n",
    "\n",
    "6. **Generate Content**: If the content seems to be missing, make a logical inference based on the surrounding text to construct a plausible summary for each of these sections.\n",
    "\n",
    "**Input Data:**\n",
    "{full_text}\n",
    "\"\"\"\n",
    "\n",
    "# Send the request to the model\n",
    "res = client.chat(\n",
    "    model=\"llama3.1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    options={\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    ")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a737d678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.1', 'created_at': '2024-08-28T09:36:26.8910967Z', 'message': {'role': 'assistant', 'content': 'Here are the summaries of each section:\\n\\n**Introduction:**\\n\\nThe study aims to address a research problem in the field of machine learning and data analysis. The background information suggests that current algorithms have limitations when it comes to generating samples for various domains such as images, molecules, and enhancers. The objectives of the study are to develop new methods for generating samples using histograms and to compare their performance with existing algorithms. The research questions being addressed include how well these new methods can generate high-quality samples and whether they outperform existing algorithms in terms of accuracy and efficiency.\\n\\n**Methodology:**\\n\\nThe researchers used a combination of machine learning techniques, including pre-training, best-N, deep SVDD (DSVDD), and Monte Carlo regression. They developed a novel method for generating samples using histograms, which was then compared with the performance of existing algorithms. The data collection involved generating additional samples using the new methods, and analysis techniques included Pearson correlation on test datasets. The researchers also used specific tools such as Python libraries to implement their methods.\\n\\n**Results and Discussion:**\\n\\nThe study found that the new method for generating samples using histograms outperformed existing algorithms in terms of accuracy and efficiency. The results showed that the top 10 and 20 quantiles of generated samples were higher for the novel method compared to other algorithms. The performance of value function training was also improved using Monte Carlo regression. The findings suggest that the new method can generate high-quality samples, particularly for domains such as images and molecules. However, further research is needed to explore potential limitations and implications of these results.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 228004248400, 'load_duration': 13714791000, 'prompt_eval_count': 1026, 'prompt_eval_duration': 19514359000, 'eval_count': 316, 'eval_duration': 194759430000}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22fcb49c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Extract and display the summaries from the response\\nresponse_content = res[\\'message\\'][\\'content\\']\\n\\n# Split the response into sections\\nimport re\\n\\n\\n# Use regular expressions to extract sections\\nintroduction = re.search(r\\'\\\\*\\\\*Introduction:\\\\*\\\\*\\n\\n(.*?)(?=\\n\\n\\\\*\\\\*|$)\\', response_content, re.DOTALL)\\nmethodology = re.search(r\\'\\\\*\\\\*Methodology:\\\\*\\\\*\\n\\n(.*?)(?=\\n\\n\\\\*\\\\*|$)\\', response_content, re.DOTALL)\\nresults_and_discussion = re.search(r\\'\\\\*\\\\*Results and Discussion:\\\\*\\\\*\\n\\n(.*?)(?=\\n\\n\\\\*\\\\*|$)\\', response_content, re.DOTALL)\\n\\n# Store the extracted text in variables\\nintroduction_summary = introduction.group(1).strip() if introduction else \"Introduction section not available.\"\\nmethodology_summary = methodology.group(1).strip() if methodology else \"Methodology section not available.\"\\nresults_and_discussion_summary = results_and_discussion.group(1).strip() if results_and_discussion else \"Results and Discussion section not available.\"\\n\\n# Display the summaries\\nprint(\"Introduction:\\n\")\\nprint(introduction_summary)\\nprint(\"\\nMethodology:\\n\")\\nprint(methodology_summary)\\nprint(\"\\nResults and Discussion:\\n\")\\nprint(results_and_discussion_summary)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Extract and display the summaries from the response\n",
    "response_content = res['message']['content']\n",
    "\n",
    "# Split the response into sections\n",
    "import re\n",
    "\n",
    "\n",
    "# Use regular expressions to extract sections\n",
    "introduction = re.search(r'\\*\\*Introduction:\\*\\*\\n\\n(.*?)(?=\\n\\n\\*\\*|$)', response_content, re.DOTALL)\n",
    "methodology = re.search(r'\\*\\*Methodology:\\*\\*\\n\\n(.*?)(?=\\n\\n\\*\\*|$)', response_content, re.DOTALL)\n",
    "results_and_discussion = re.search(r'\\*\\*Results and Discussion:\\*\\*\\n\\n(.*?)(?=\\n\\n\\*\\*|$)', response_content, re.DOTALL)\n",
    "\n",
    "# Store the extracted text in variables\n",
    "introduction_summary = introduction.group(1).strip() if introduction else \"Introduction section not available.\"\n",
    "methodology_summary = methodology.group(1).strip() if methodology else \"Methodology section not available.\"\n",
    "results_and_discussion_summary = results_and_discussion.group(1).strip() if results_and_discussion else \"Results and Discussion section not available.\"\n",
    "\n",
    "# Display the summaries\n",
    "print(\"Introduction:\\n\")\n",
    "print(introduction_summary)\n",
    "print(\"\\nMethodology:\\n\")\n",
    "print(methodology_summary)\n",
    "print(\"\\nResults and Discussion:\\n\")\n",
    "print(results_and_discussion_summary)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "627b510f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction:\n",
      "\n",
      "The study aims to address a research problem in the field of machine learning and data analysis. The background information suggests that current algorithms have limitations when it comes to generating samples for various domains such as images, molecules, and enhancers. The objectives of the study are to develop new methods for generating samples using histograms and to compare their performance with existing algorithms. The research questions being addressed include how well these new methods can generate high-quality samples and whether they outperform existing algorithms in terms of accuracy and efficiency.\n",
      "\n",
      "Methodology:\n",
      "\n",
      "The researchers used a combination of machine learning techniques, including pre-training, best-N, deep SVDD (DSVDD), and Monte Carlo regression. They developed a novel method for generating samples using histograms, which was then compared with the performance of existing algorithms. The data collection involved generating additional samples using the new methods, and analysis techniques included Pearson correlation on test datasets. The researchers also used specific tools such as Python libraries to implement their methods.\n",
      "\n",
      "Results and Discussion:\n",
      "\n",
      "The study found that the new method for generating samples using histograms outperformed existing algorithms in terms of accuracy and efficiency. The results showed that the top 10 and 20 quantiles of generated samples were higher for the novel method compared to other algorithms. The performance of value function training was also improved using Monte Carlo regression. The findings suggest that the new method can generate high-quality samples, particularly for domains such as images and molecules. However, further research is needed to explore potential limitations and implications of these results.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Extract and display the summaries from the response\n",
    "response_content = res['message']['content']\n",
    "\n",
    "# Extract sections using more robust regular expressions\n",
    "introduction = re.search(r'\\*\\*Introduction:\\*\\*\\s*(.*?)(?=\\n\\n\\*\\*|$)', response_content, re.DOTALL)\n",
    "methodology = re.search(r'\\*\\*Methodology:\\*\\*\\s*(.*?)(?=\\n\\n\\*\\*|$)', response_content, re.DOTALL)\n",
    "results_and_discussion = re.search(r'\\*\\*Results and Discussion:\\*\\*\\s*(.*?)(?=\\n\\n\\*\\*|$)', response_content, re.DOTALL)\n",
    "\n",
    "# Function to clean and format the extracted text\n",
    "def clean_text(text):\n",
    "    if text:\n",
    "        cleaned = re.split(r'\\n\\s*Note:', text.group(1).strip(), flags=re.IGNORECASE)[0]\n",
    "        return cleaned.strip()\n",
    "    return \"Section not available.\"\n",
    "\n",
    "# Store the extracted and cleaned text in variables\n",
    "introduction_summary = clean_text(introduction)\n",
    "methodology_summary = clean_text(methodology)\n",
    "results_and_discussion_summary = clean_text(results_and_discussion)\n",
    "\n",
    "# Display the summaries\n",
    "print(\"Introduction:\\n\")\n",
    "print(introduction_summary)\n",
    "print(\"\\nMethodology:\\n\")\n",
    "print(methodology_summary)\n",
    "print(\"\\nResults and Discussion:\\n\")\n",
    "print(results_and_discussion_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3611c6d2",
   "metadata": {},
   "source": [
    "# First version of only text on poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54aa0c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import matplotlib.pyplot as plt\\nfrom PIL import Image, ImageDraw, ImageFont\\nimport textwrap\\n\\n# Create a new image with a white background\\nimg = Image.new(\\'RGB\\', (3000, 2000), color = (255, 255, 255))\\n\\n# Get a drawing context\\nd = ImageDraw.Draw(img)\\n\\n# Use a monospaced font\\ntitle_font = ImageFont.truetype(\\'arial.ttf\\', 40)\\nbody_font = ImageFont.truetype(\\'arial.ttf\\', 20)\\n\\n# Define the column widths\\ncolumn_width = img.width // 3\\n\\n# Define the title and body text\\ntitles = [\"Introduction\", \"Methodology\", \"Results and Discussion\"]\\nsummaries = [introduction_summary, methodology_summary, results_and_discussion_summary]\\n\\n# Draw the titles and summaries\\nfor i, title in enumerate(titles):\\n    # Draw the title\\n    title_width, title_height = title_font.getsize(title)\\n    title_x = (i * column_width) + ((column_width - title_width) // 2)\\n    title_y = 50\\n    d.text((title_x, title_y), title, font=title_font, fill=(0, 0, 0))\\n\\n    # Wrap the summary text to fit within the column\\n    wrapped_summary = textwrap.fill(summaries[i], width=40)\\n\\n    # Draw the summary\\n    summary_y = title_y + title_height + 50\\n    d.text((i * column_width + 50, summary_y), wrapped_summary, font=body_font, fill=(0, 0, 0))\\n\\n# Draw the column separators\\nd.line([(column_width, 0), (column_width, img.height)], fill=(0, 0, 0), width=5)\\nd.line([(column_width * 2, 0), (column_width * 2, img.height)], fill=(0, 0, 0), width=5)\\n\\n# Save the image\\nimg.save(\\'poster.png\\')'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "\n",
    "# Create a new image with a white background\n",
    "img = Image.new('RGB', (3000, 2000), color = (255, 255, 255))\n",
    "\n",
    "# Get a drawing context\n",
    "d = ImageDraw.Draw(img)\n",
    "\n",
    "# Use a monospaced font\n",
    "title_font = ImageFont.truetype('arial.ttf', 40)\n",
    "body_font = ImageFont.truetype('arial.ttf', 20)\n",
    "\n",
    "# Define the column widths\n",
    "column_width = img.width // 3\n",
    "\n",
    "# Define the title and body text\n",
    "titles = [\"Introduction\", \"Methodology\", \"Results and Discussion\"]\n",
    "summaries = [introduction_summary, methodology_summary, results_and_discussion_summary]\n",
    "\n",
    "# Draw the titles and summaries\n",
    "for i, title in enumerate(titles):\n",
    "    # Draw the title\n",
    "    title_width, title_height = title_font.getsize(title)\n",
    "    title_x = (i * column_width) + ((column_width - title_width) // 2)\n",
    "    title_y = 50\n",
    "    d.text((title_x, title_y), title, font=title_font, fill=(0, 0, 0))\n",
    "\n",
    "    # Wrap the summary text to fit within the column\n",
    "    wrapped_summary = textwrap.fill(summaries[i], width=40)\n",
    "\n",
    "    # Draw the summary\n",
    "    summary_y = title_y + title_height + 50\n",
    "    d.text((i * column_width + 50, summary_y), wrapped_summary, font=body_font, fill=(0, 0, 0))\n",
    "\n",
    "# Draw the column separators\n",
    "d.line([(column_width, 0), (column_width, img.height)], fill=(0, 0, 0), width=5)\n",
    "d.line([(column_width * 2, 0), (column_width * 2, img.height)], fill=(0, 0, 0), width=5)\n",
    "\n",
    "# Save the image\n",
    "img.save('poster.png')'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2f441",
   "metadata": {},
   "source": [
    "# Second version : Text + Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55fd6170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import matplotlib.pyplot as plt\\nfrom PIL import Image, ImageDraw, ImageFont\\nimport textwrap\\nimport os\\n\\ndef create_scientific_poster(summaries, output_directory, output_path):\\n    # Create a new image with a white background\\n    img = Image.new(\\'RGB\\', (3000, 2000), color = (255, 255, 255))\\n\\n    # Get a drawing context\\n    d = ImageDraw.Draw(img)\\n\\n    # Use a monospaced font\\n    title_font = ImageFont.truetype(\\'arial.ttf\\', 40)\\n    body_font = ImageFont.truetype(\\'arial.ttf\\', 20)\\n\\n    # Define the column widths\\n    column_width = img.width // 3\\n\\n    # Define the title and body text\\n    titles = [\"Introduction\", \"Methodology\", \"Results and Discussion\"]\\n\\n    # Draw the titles, summaries, and images\\n    for i, title in enumerate(titles):\\n        # Draw the title\\n        title_width, title_height = title_font.getsize(title)\\n        title_x = (i * column_width) + ((column_width - title_width) // 2)\\n        title_y = 50\\n        d.text((title_x, title_y), title, font=title_font, fill=(0, 0, 0))\\n\\n        # Wrap the summary text to fit within the column\\n        wrapped_summary = textwrap.fill(summaries[i], width=40)\\n\\n        # Draw the summary\\n        summary_y = title_y + title_height + 50\\n        d.multiline_text((i * column_width + 50, summary_y), wrapped_summary, font=body_font, fill=(0, 0, 0))\\n\\n        # Calculate the height of the summary text\\n        summary_height = d.multiline_textsize(wrapped_summary, font=body_font)[1]\\n\\n        # Add images from the corresponding folder\\n        image_folder = os.path.join(output_directory, title)\\n        image_y = summary_y + summary_height + 50\\n        if os.path.exists(image_folder):\\n            for image_file in os.listdir(image_folder):\\n                if image_file.lower().endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.gif\\')):\\n                    image_path = os.path.join(image_folder, image_file)\\n                    with Image.open(image_path) as section_image:\\n                        # Resize the image to fit the column width while maintaining aspect ratio\\n                        section_image.thumbnail((column_width - 100, 400))\\n                        image_x = i * column_width + (column_width - section_image.width) // 2\\n                        img.paste(section_image, (image_x, image_y))\\n                        image_y += section_image.height + 20  # Add some space between images\\n\\n    # Draw the column separators\\n    d.line([(column_width, 0), (column_width, img.height)], fill=(0, 0, 0), width=5)\\n    d.line([(column_width * 2, 0), (column_width * 2, img.height)], fill=(0, 0, 0), width=5)\\n\\n    # Save the image\\n    img.save(output_path)\\n    print(f\"Poster saved as {output_path}\")\\n\\n# Usage example\\nsummaries = [introduction_summary, methodology_summary, results_and_discussion_summary]\\noutput_directory = \"extracted_images\"\\noutput_path = \\'scientific_poster.png\\'\\n\\ncreate_scientific_poster(summaries, output_directory, output_path)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "def create_scientific_poster(summaries, output_directory, output_path):\n",
    "    # Create a new image with a white background\n",
    "    img = Image.new('RGB', (3000, 2000), color = (255, 255, 255))\n",
    "\n",
    "    # Get a drawing context\n",
    "    d = ImageDraw.Draw(img)\n",
    "\n",
    "    # Use a monospaced font\n",
    "    title_font = ImageFont.truetype('arial.ttf', 40)\n",
    "    body_font = ImageFont.truetype('arial.ttf', 20)\n",
    "\n",
    "    # Define the column widths\n",
    "    column_width = img.width // 3\n",
    "\n",
    "    # Define the title and body text\n",
    "    titles = [\"Introduction\", \"Methodology\", \"Results and Discussion\"]\n",
    "\n",
    "    # Draw the titles, summaries, and images\n",
    "    for i, title in enumerate(titles):\n",
    "        # Draw the title\n",
    "        title_width, title_height = title_font.getsize(title)\n",
    "        title_x = (i * column_width) + ((column_width - title_width) // 2)\n",
    "        title_y = 50\n",
    "        d.text((title_x, title_y), title, font=title_font, fill=(0, 0, 0))\n",
    "\n",
    "        # Wrap the summary text to fit within the column\n",
    "        wrapped_summary = textwrap.fill(summaries[i], width=40)\n",
    "\n",
    "        # Draw the summary\n",
    "        summary_y = title_y + title_height + 50\n",
    "        d.multiline_text((i * column_width + 50, summary_y), wrapped_summary, font=body_font, fill=(0, 0, 0))\n",
    "\n",
    "        # Calculate the height of the summary text\n",
    "        summary_height = d.multiline_textsize(wrapped_summary, font=body_font)[1]\n",
    "\n",
    "        # Add images from the corresponding folder\n",
    "        image_folder = os.path.join(output_directory, title)\n",
    "        image_y = summary_y + summary_height + 50\n",
    "        if os.path.exists(image_folder):\n",
    "            for image_file in os.listdir(image_folder):\n",
    "                if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "                    image_path = os.path.join(image_folder, image_file)\n",
    "                    with Image.open(image_path) as section_image:\n",
    "                        # Resize the image to fit the column width while maintaining aspect ratio\n",
    "                        section_image.thumbnail((column_width - 100, 400))\n",
    "                        image_x = i * column_width + (column_width - section_image.width) // 2\n",
    "                        img.paste(section_image, (image_x, image_y))\n",
    "                        image_y += section_image.height + 20  # Add some space between images\n",
    "\n",
    "    # Draw the column separators\n",
    "    d.line([(column_width, 0), (column_width, img.height)], fill=(0, 0, 0), width=5)\n",
    "    d.line([(column_width * 2, 0), (column_width * 2, img.height)], fill=(0, 0, 0), width=5)\n",
    "\n",
    "    # Save the image\n",
    "    img.save(output_path)\n",
    "    print(f\"Poster saved as {output_path}\")\n",
    "\n",
    "# Usage example\n",
    "summaries = [introduction_summary, methodology_summary, results_and_discussion_summary]\n",
    "output_directory = \"extracted_images\"\n",
    "output_path = 'scientific_poster.png'\n",
    "\n",
    "create_scientific_poster(summaries, output_directory, output_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d413c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c5a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a568be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77c90cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:52: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  title_width, title_height = title_font.getsize(title)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:52: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  title_width, title_height = title_font.getsize(title)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:91: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  section_image.thumbnail(new_size, Image.ANTIALIAS)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:52: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
      "  title_width, title_height = title_font.getsize(title)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:60: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n",
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:65: DeprecationWarning: textsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use textbbox or textlength instead.\n",
      "  line_width, line_height = d.textsize(line, font=body_font)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tarun\\AppData\\Local\\Temp\\ipykernel_27484\\890148101.py:91: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  section_image.thumbnail(new_size, Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poster saved as scientific_poster.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageStat\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "def select_images(image_folder, num_images=3):\n",
    "    if not os.path.exists(image_folder):\n",
    "        return []\n",
    "\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n",
    "    \n",
    "    if not image_files:\n",
    "        return []\n",
    "\n",
    "    def get_image_quality(image_path):\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "            stat = ImageStat.Stat(img)\n",
    "            sharpness = stat.mean[:3]  # Mean RGB values as a proxy for image sharpness\n",
    "            resolution = width * height\n",
    "            return resolution, sharpness\n",
    "\n",
    "    image_quality = [(img, get_image_quality(os.path.join(image_folder, img))) for img in image_files]\n",
    "    sorted_images = sorted(image_quality, key=lambda x: (-x[1][0], x[1][1]))  # Sort by resolution and sharpness\n",
    "\n",
    "    selected_images = [img for img, _ in sorted_images[:num_images]]\n",
    "    return selected_images\n",
    "def create_scientific_poster(summaries, output_directory, output_path):\n",
    "    # Create a new image with a white background\n",
    "    img = Image.new('RGB', (3000, 2000), color=(255, 255, 255))\n",
    "    d = ImageDraw.Draw(img)\n",
    "\n",
    "    # Use a monospaced font\n",
    "    title_font = ImageFont.truetype('arialbd.ttf', 80)\n",
    "    body_font = ImageFont.truetype('arial.ttf', 30)\n",
    "\n",
    "    # Define layout parameters\n",
    "    column_width = img.width // 3\n",
    "    heading_row_height = 150\n",
    "    heading_color = (100, 149, 237)  # Cornflower blue\n",
    "    section_background_color = (224, 238, 255)  # Light pastel blue\n",
    "\n",
    "    # Fill the heading row and section backgrounds\n",
    "    d.rectangle([(0, 0), (img.width, heading_row_height)], fill=heading_color)\n",
    "    for i in range(3):\n",
    "        d.rectangle([(i * column_width, heading_row_height), ((i + 1) * column_width, img.height)], fill=section_background_color)\n",
    "\n",
    "    titles = [\"Introduction\", \"Methodology\", \"Results and Discussion\"]\n",
    "\n",
    "    for i, title in enumerate(titles):\n",
    "        # Draw the title\n",
    "        title_width, title_height = title_font.getsize(title)\n",
    "        title_x = (i * column_width) + ((column_width - title_width) // 2)\n",
    "        title_y = (heading_row_height - title_height) // 2\n",
    "        d.text((title_x, title_y), title, font=title_font, fill=(255, 255, 255))\n",
    "\n",
    "        # Wrap and draw the summary text\n",
    "        wrapped_summary = textwrap.fill(summaries[i], width=30)\n",
    "        summary_lines = wrapped_summary.split('\\n')\n",
    "        max_line_width = max([d.textsize(line, font=body_font)[0] for line in summary_lines])\n",
    "        summary_x = i * column_width + (column_width - max_line_width) // 2\n",
    "        summary_y = heading_row_height + 50\n",
    "\n",
    "        for line in summary_lines:\n",
    "            line_width, line_height = d.textsize(line, font=body_font)\n",
    "            d.text((summary_x, summary_y), line, font=body_font, fill=(0, 0, 0))\n",
    "            summary_y += line_height + 10\n",
    "\n",
    "        # Calculate available space for images\n",
    "        image_start_y = summary_y + 50\n",
    "        available_height = img.height - image_start_y - 50\n",
    "\n",
    "        # Add images\n",
    "        image_folder = os.path.join(output_directory, title)\n",
    "        selected_images = select_images(image_folder, num_images=3)\n",
    "\n",
    "        if selected_images:\n",
    "            # Calculate image sizes\n",
    "            image_width = (column_width - 100) // 2\n",
    "            image_height = available_height // 2\n",
    "\n",
    "            for j, image_file in enumerate(selected_images):\n",
    "                image_path = os.path.join(image_folder, image_file)\n",
    "                with Image.open(image_path) as section_image:\n",
    "                    # Resize image\n",
    "                    if j < 2:\n",
    "                        new_size = (image_width, image_height)\n",
    "                    else:\n",
    "                        new_size = (column_width - 100, image_height)\n",
    "                    \n",
    "                    section_image.thumbnail(new_size, Image.ANTIALIAS)\n",
    "\n",
    "                    # Calculate position\n",
    "                    if j < 2:\n",
    "                        image_x = i * column_width + 50 + (j * (image_width + 10))\n",
    "                        image_y = image_start_y\n",
    "                    else:\n",
    "                        image_x = i * column_width + 50\n",
    "                        image_y = image_start_y + image_height + 10\n",
    "\n",
    "                    # Paste the image\n",
    "                    img.paste(section_image, (image_x, image_y))\n",
    "\n",
    "    # Draw column separators\n",
    "    d.line([(column_width, 0), (column_width, img.height)], fill=(0, 0, 0), width=5)\n",
    "    d.line([(column_width * 2, 0), (column_width * 2, img.height)], fill=(0, 0, 0), width=5)\n",
    "\n",
    "    # Save the image\n",
    "    img.save(output_path)\n",
    "    print(f\"Poster saved as {output_path}\")\n",
    "\n",
    "# Usage example\n",
    "summaries = [introduction_summary, methodology_summary, results_and_discussion_summary]\n",
    "output_directory = \"extracted_images\"\n",
    "output_path = 'scientific_poster.png'\n",
    "\n",
    "create_scientific_poster(summaries, output_directory, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d247f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918d242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe51d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "589351d7",
   "metadata": {},
   "source": [
    "## 3rd version : Text + image compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14feeb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import matplotlib.pyplot as plt\\nfrom PIL import Image, ImageDraw, ImageFont\\nimport textwrap\\nimport os\\nimport random\\n\\ndef create_image_collage(image_folder, max_images=4, collage_size=(400, 400)):\\n    images = []\\n    for image_file in os.listdir(image_folder):\\n        if image_file.lower().endswith((\\'.png\\', \\'.jpg\\', \\'.jpeg\\', \\'.gif\\')):\\n            image_path = os.path.join(image_folder, image_file)\\n            images.append(Image.open(image_path))\\n    \\n    if not images:\\n        return None\\n    \\n    # Randomly select images if there are more than max_images\\n    if len(images) > max_images:\\n        images = random.sample(images, max_images)\\n    \\n    collage = Image.new(\\'RGB\\', collage_size, (255, 255, 255))\\n    \\n    # Calculate the size for each image in the collage\\n    img_width = collage_size[0] // 2\\n    img_height = collage_size[1] // 2\\n    \\n    for i, img in enumerate(images):\\n        # Resize the image to fit in the collage\\n        img.thumbnail((img_width, img_height))\\n        \\n        # Calculate position\\n        x = (i % 2) * img_width\\n        y = (i // 2) * img_height\\n        \\n        # Paste the image into the collage\\n        collage.paste(img, (x, y))\\n    \\n    return collage\\n\\ndef create_scientific_poster(summaries, output_directory, output_path):\\n    # Create a new image with a white background\\n    img = Image.new(\\'RGB\\', (3000, 2000), color = (255, 255, 255))\\n\\n    # Get a drawing context\\n    d = ImageDraw.Draw(img)\\n\\n    # Use a monospaced font\\n    title_font = ImageFont.truetype(\\'arial.ttf\\', 40)\\n    body_font = ImageFont.truetype(\\'arial.ttf\\', 20)\\n\\n    # Define the column widths\\n    column_width = img.width // 3\\n\\n    # Define the title and body text\\n    titles = [\"Introduction\", \"Methodology\", \"Results and Discussion\"]\\n\\n    # Draw the titles, summaries, and images\\n    for i, title in enumerate(titles):\\n        # Draw the title\\n        title_width, title_height = title_font.getsize(title)\\n        title_x = (i * column_width) + ((column_width - title_width) // 2)\\n        title_y = 50\\n        d.text((title_x, title_y), title, font=title_font, fill=(0, 0, 0))\\n\\n        # Wrap the summary text to fit within the column\\n        wrapped_summary = textwrap.fill(summaries[i], width=40)\\n\\n        # Draw the summary\\n        summary_y = title_y + title_height + 50\\n        d.multiline_text((i * column_width + 50, summary_y), wrapped_summary, font=body_font, fill=(0, 0, 0))\\n\\n        # Calculate the height of the summary text\\n        summary_height = d.multiline_textsize(wrapped_summary, font=body_font)[1]\\n\\n        # Create and add image collage\\n        image_folder = os.path.join(output_directory, title)\\n        if os.path.exists(image_folder):\\n            collage = create_image_collage(image_folder)\\n            if collage:\\n                collage_y = summary_y + summary_height + 50\\n                collage_x = i * column_width + (column_width - collage.width) // 2\\n                img.paste(collage, (collage_x, collage_y))\\n\\n    # Draw the column separators\\n    d.line([(column_width, 0), (column_width, img.height)], fill=(0, 0, 0), width=5)\\n    d.line([(column_width * 2, 0), (column_width * 2, img.height)], fill=(0, 0, 0), width=5)\\n\\n    # Save the image\\n    img.save(output_path)\\n    print(f\"Poster saved as {output_path}\")\\n\\n# Usage example\\nsummaries = [introduction_summary, methodology_summary, results_and_discussion_summary]\\noutput_directory = \"extracted_images\"\\noutput_path = \\'scientific_poster.png\\'\\n\\ncreate_scientific_poster(summaries, output_directory, output_path)'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "import os\n",
    "import random\n",
    "\n",
    "def create_image_collage(image_folder, max_images=4, collage_size=(400, 400)):\n",
    "    images = []\n",
    "    for image_file in os.listdir(image_folder):\n",
    "        if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "            image_path = os.path.join(image_folder, image_file)\n",
    "            images.append(Image.open(image_path))\n",
    "    \n",
    "    if not images:\n",
    "        return None\n",
    "    \n",
    "    # Randomly select images if there are more than max_images\n",
    "    if len(images) > max_images:\n",
    "        images = random.sample(images, max_images)\n",
    "    \n",
    "    collage = Image.new('RGB', collage_size, (255, 255, 255))\n",
    "    \n",
    "    # Calculate the size for each image in the collage\n",
    "    img_width = collage_size[0] // 2\n",
    "    img_height = collage_size[1] // 2\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        # Resize the image to fit in the collage\n",
    "        img.thumbnail((img_width, img_height))\n",
    "        \n",
    "        # Calculate position\n",
    "        x = (i % 2) * img_width\n",
    "        y = (i // 2) * img_height\n",
    "        \n",
    "        # Paste the image into the collage\n",
    "        collage.paste(img, (x, y))\n",
    "    \n",
    "    return collage\n",
    "\n",
    "def create_scientific_poster(summaries, output_directory, output_path):\n",
    "    # Create a new image with a white background\n",
    "    img = Image.new('RGB', (3000, 2000), color = (255, 255, 255))\n",
    "\n",
    "    # Get a drawing context\n",
    "    d = ImageDraw.Draw(img)\n",
    "\n",
    "    # Use a monospaced font\n",
    "    title_font = ImageFont.truetype('arial.ttf', 40)\n",
    "    body_font = ImageFont.truetype('arial.ttf', 20)\n",
    "\n",
    "    # Define the column widths\n",
    "    column_width = img.width // 3\n",
    "\n",
    "    # Define the title and body text\n",
    "    titles = [\"Introduction\", \"Methodology\", \"Results and Discussion\"]\n",
    "\n",
    "    # Draw the titles, summaries, and images\n",
    "    for i, title in enumerate(titles):\n",
    "        # Draw the title\n",
    "        title_width, title_height = title_font.getsize(title)\n",
    "        title_x = (i * column_width) + ((column_width - title_width) // 2)\n",
    "        title_y = 50\n",
    "        d.text((title_x, title_y), title, font=title_font, fill=(0, 0, 0))\n",
    "\n",
    "        # Wrap the summary text to fit within the column\n",
    "        wrapped_summary = textwrap.fill(summaries[i], width=40)\n",
    "\n",
    "        # Draw the summary\n",
    "        summary_y = title_y + title_height + 50\n",
    "        d.multiline_text((i * column_width + 50, summary_y), wrapped_summary, font=body_font, fill=(0, 0, 0))\n",
    "\n",
    "        # Calculate the height of the summary text\n",
    "        summary_height = d.multiline_textsize(wrapped_summary, font=body_font)[1]\n",
    "\n",
    "        # Create and add image collage\n",
    "        image_folder = os.path.join(output_directory, title)\n",
    "        if os.path.exists(image_folder):\n",
    "            collage = create_image_collage(image_folder)\n",
    "            if collage:\n",
    "                collage_y = summary_y + summary_height + 50\n",
    "                collage_x = i * column_width + (column_width - collage.width) // 2\n",
    "                img.paste(collage, (collage_x, collage_y))\n",
    "\n",
    "    # Draw the column separators\n",
    "    d.line([(column_width, 0), (column_width, img.height)], fill=(0, 0, 0), width=5)\n",
    "    d.line([(column_width * 2, 0), (column_width * 2, img.height)], fill=(0, 0, 0), width=5)\n",
    "\n",
    "    # Save the image\n",
    "    img.save(output_path)\n",
    "    print(f\"Poster saved as {output_path}\")\n",
    "\n",
    "# Usage example\n",
    "summaries = [introduction_summary, methodology_summary, results_and_discussion_summary]\n",
    "output_directory = \"extracted_images\"\n",
    "output_path = 'scientific_poster.png'\n",
    "\n",
    "create_scientific_poster(summaries, output_directory, output_path)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd003b6d",
   "metadata": {},
   "source": [
    "# Generating the poster using stable diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a722e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "def load_local_images(directory):\n",
    "    images = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                images.append(Image.open(image_path).convert(\"RGB\"))\n",
    "    return images\n",
    "\n",
    "# Set the path to your local image directories\n",
    "image_directory = \"extracted_images\"\n",
    "\n",
    "# Load all images from the specified directory\n",
    "images = load_local_images(image_directory)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef501ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=227x253>,\n",
       " <PIL.Image.Image image mode=RGB size=500x209>,\n",
       " <PIL.Image.Image image mode=RGB size=500x207>,\n",
       " <PIL.Image.Image image mode=RGB size=500x395>,\n",
       " <PIL.Image.Image image mode=RGB size=500x256>,\n",
       " <PIL.Image.Image image mode=RGB size=500x398>,\n",
       " <PIL.Image.Image image mode=RGB size=500x398>,\n",
       " <PIL.Image.Image image mode=RGB size=500x298>,\n",
       " <PIL.Image.Image image mode=RGB size=500x298>,\n",
       " <PIL.Image.Image image mode=RGB size=500x298>,\n",
       " <PIL.Image.Image image mode=RGB size=500x298>,\n",
       " <PIL.Image.Image image mode=RGB size=500x396>,\n",
       " <PIL.Image.Image image mode=RGB size=500x254>,\n",
       " <PIL.Image.Image image mode=RGB size=500x262>,\n",
       " <PIL.Image.Image image mode=RGB size=500x262>,\n",
       " <PIL.Image.Image image mode=RGB size=500x262>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=512x512>,\n",
       " <PIL.Image.Image image mode=RGB size=500x279>,\n",
       " <PIL.Image.Image image mode=RGB size=500x284>,\n",
       " <PIL.Image.Image image mode=RGB size=500x150>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a7c6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from PIL import Image, ImageDraw\n",
    "\n",
    "def add_border(image, border_size=10, border_color=(0, 0, 0)):\n",
    "    # Create a new image with border\n",
    "    bordered_image = Image.new('RGB', \n",
    "                               (image.width + 2 * border_size, image.height + 2 * border_size), \n",
    "                               border_color)\n",
    "    bordered_image.paste(image, (border_size, border_size))\n",
    "    return bordered_image\n",
    "def create_composite_image_with_borders(images, border_size=10, border_color=(0, 0, 0)):\n",
    "    if not images:\n",
    "        raise ValueError(\"No images to combine.\")\n",
    "    \n",
    "    # Add borders to each image\n",
    "    bordered_images = [add_border(img, border_size, border_color) for img in images]\n",
    "\n",
    "    # Calculate width and height for the final poster\n",
    "    widths, heights = zip(*(i.size for i in bordered_images))\n",
    "    \n",
    "    # Create a new image with the maximum width and the sum of heights\n",
    "    total_width = max(widths)\n",
    "    total_height = sum(heights)\n",
    "    \n",
    "    new_image = Image.new('RGB', (total_width, total_height), (255, 255, 255))\n",
    "    \n",
    "    y_offset = 0\n",
    "    for img in bordered_images:\n",
    "        new_image.paste(img, (0, y_offset))\n",
    "        y_offset += img.height\n",
    "\n",
    "    return new_image\n",
    "# Create the composite image with borders\n",
    "composite_image = create_composite_image_with_borders(images, border_size=10, border_color=(0, 0, 0))\n",
    "composite_image.save(\"composite_image_with_borders.png\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "490ae017",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''prompt = f\"\"\"\n",
    "Create a high-quality scientific poster layout with three distinct vertical columns, each dedicated to a specific section: 'Introduction,' 'Methodology,' and 'Results and Discussion.' The poster should be visually appealing and suitable for a professional conference setting, with clean lines, clear headings, and well-organized content.\n",
    "\n",
    "- **Introduction**: In the first column, summarize the key points of the research as described below:\n",
    "  \n",
    "  _'{introduction_summary}'_\n",
    "  \n",
    "  Ensure the text is concise and informative, providing context and background for the study.\n",
    "\n",
    "- **Methodology**: The second column should focus on the research methods, summarizing the essential steps and approaches as described below:\n",
    "  \n",
    "  _'{methodology_summary}'_\n",
    "  \n",
    "  Include any necessary diagrams or flowcharts that clarify the experimental setup or procedures.\n",
    "\n",
    "- **Results and Discussion**: In the third column, present the key findings and analysis as detailed below:\n",
    "  \n",
    "  _'{results_and_discussion_summary}'_\n",
    "  \n",
    "  Highlight any significant trends, correlations, or conclusions drawn from the data.\n",
    "\n",
    "In each column, ensure the provided images from the image given is prominently placed exactly where they are relevant, without any alterations. The images should be seamlessly integrated into the design without any modifications (They should, aligned with the text, and maintaining their original clarity and quality. Use appropriate captions under each image to describe its relevance to the section's content.\n",
    "\n",
    "The overall design should be cohesive, with a consistent color scheme and font choice that enhances readability while maintaining a professional and academic tone.\n",
    "\"\"\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12a8a7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "     ---------------------------------------- 0.0/315.1 kB ? eta -:--:--\n",
      "     -------------------------------------  307.2/315.1 kB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 315.1/315.1 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from accelerate) (1.22.4)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from accelerate) (0.24.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.10.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.28.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tarun\\anaconda3\\envs\\whisper\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.33.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aeeec9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78553e7d96c491fa63202cfc740a7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"summarize the key points of the research as described below : _' the introduction provides background information on the importance of generating high - quality samples in various domains such as images, molecules, and enhancers. it highlights the challenges of current methods and the need for a more efficient approach. the research aims to address this problem by employing a novel method that uses convgru as the value function model. the objectives are to evaluate the performance of generated samples using histograms and compare it with existing algorithms.'_ ensure the text is concise and informative, providing context and background for the study. - ** methodology **: the second column should focus on the research methods, summarizing the essential steps and approaches as described below : _' the methodology section explains how the researchers employed a novel method to generate high - quality samples in various domains. they used convgru as the value function model, which is a type of recurrent neural network ( rnn ) that combines convolutional and recurrent layers. the researchers also used monte carlo regression to train the value function. additionally, they compared their results with existing algorithms such as best - n, svdd - mc, and svdd - pm.'_ include any necessary diagrams or flowcharts that clarify the experimental setup or procedures. - ** results and discussion **: in the third column, present the key findings and analysis as detailed below : _' the results section presents the top 1 0 and 2 0 quantiles of generated samples for each algorithm in various domains. the data shows that the proposed method outperforms existing algorithms in terms of sample quality. the discussion interprets the results by highlighting the advantages of using convgru as the value function model. the researchers also provide additional generated samples to demonstrate the effectiveness of their method.'_ highlight any significant trends, correlations, or conclusions drawn from the data. in each column, ensure the provided images from the image given is prominently placed exactly where they are relevant, without any alterations. the images should be seamlessly integrated into the design without any modifications ( they should, aligned with the text, and maintaining their original clarity and quality. use appropriate captions under each image to describe its relevance to the section's content. the overall design should be cohesive, with a consistent color scheme and font choice that enhances readability while maintaining a professional and academic tone.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22bdc1df0fe4f498310b32c7458778b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final poster image: final_poster.png\n"
     ]
    }
   ],
   "source": [
    "'''# Load the Stable Diffusion model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "pipe = pipe.to(device)\n",
    "# Generate the final poster\n",
    "composite_image_path = \"composite_image_with_borders.png\"\n",
    "composite_image = Image.open(composite_image_path).convert(\"RGB\")\n",
    "\n",
    "# Run Stable Diffusion\n",
    "result = pipe(prompt=prompt, init_image=composite_image, strength=0.75, guidance_scale=9,height=720, width=480).images[0]\n",
    "result.save(\"final_poster.png\")\n",
    "print(\"Saved final poster image: final_poster.png\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb075bf2",
   "metadata": {},
   "source": [
    "# Making bland poster beautiful using stable diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aede0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be56df3b77744651876df1d3f444558a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tarun\\Anaconda3\\envs\\Whisper\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de6cc4ab75946df994e6f9ef1984f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final poster image: final_poster.png\n"
     ]
    }
   ],
   "source": [
    "'''import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# Load the Stable Diffusion model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "pipe = pipe.to(device)\n",
    "# Generate the final poster\n",
    "composite_image_path = \"scientific_poster.png\"\n",
    "composite_image = Image.open(composite_image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Enhance the design of this scientific poster while retaining the existing text in all three sections: Introduction, Methodology, and Results & Discussion. Ensure that the text remains unchanged and perfectly readable. Improve the overall aesthetics without altering the images or content in the three columns. Focus on creating a more visually appealing and professional design that highlights the existing structure and information effectively\n",
    "\"\"\"\n",
    "\n",
    "# Run Stable Diffusion\n",
    "result = pipe(prompt=prompt, init_image=composite_image, strength=0.75, guidance_scale=9,height=720, width=480).images[0]\n",
    "result.save(\"final_poster.png\")\n",
    "print(\"Saved final poster image: final_poster.png\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f33b53",
   "metadata": {},
   "source": [
    "# Generating a poster backdrop with stable diffusion based on the research paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e985c5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdb4bc993b44443836bdd71cdf19eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261ed526b7e94b8396b94e5386911711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final poster image: final_poster.png\n"
     ]
    }
   ],
   "source": [
    "'''import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load the Stable Diffusion model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "field = 'Computer Science'\n",
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "Design a formal and professional {field} scientific poster backdrop with a clean, elegant outer border. The backdrop should be minimalistic, using a sophisticated color palette. There should be no sections, only a refined outer border, suitable for adding text and images later.\n",
    "\"\"\"\n",
    "\n",
    "# Run Stable Diffusion\n",
    "result = pipe(prompt=prompt, guidance_scale=9, height=720, width=480).images[0]\n",
    "result.save(\"final_poster.png\")\n",
    "print(\"Saved final poster image: final_poster.png\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
